{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Unsupervised learning is a method used to enable machines to classify both tangible and intangible objects without providing the machines any prior information about the objects. The things machines need to classify are varied, such as customer purchasing habits, behavioral patterns of bacteria and hacker attacks. The main idea behind unsupervised learning is to expose the machines to large volumes of varied data and allow it to learn and infer from the data. However, the machines must first be programmed to learn from data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PCA\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The main idea of principal component analysis (PCA) is to reduce the dimensionality of a data set consisting of many variables correlated with each other, either heavily or lightly, while retaining the variation present in the dataset, up to the maximum extent. The same is done by transforming the variables to a new set of variables, which are known as the principal components (or simply, the PCs) and are orthogonal, ordered such that the retention of variation present in the original variables decreases as we move down in the order. So, in this way, the 1st principal component retains maximum variation that was present in the original components. The principal components are the eigenvectors of a covariance matrix, and hence they are orthogonal.\n",
    "Importantly, the dataset on which PCA technique is to be used must be scaled. The results are also sensitive to the relative scaling. Intuitively, Principal Component Analysis can supply the user with a lower-dimensional picture, a projection or \"shadow\" of this object when viewed from its most informative viewpoint."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Technically, a principal component can be defined as a linear combination of optimally-weighted observed variables. The output of PCA are these principal components, the number of which is less than or equal to the number of original variables. Less, in case when we wish to discard or reduce the dimensions in our dataset. The PCs possess some useful properties which are listed below:\n",
    "1.The PCs are essentially the linear combinations of the original variables, the weights vector in this combination is actually the eigenvector found which in turn satisfies the principle of least squares.\n",
    "2.The PCs are orthogonal.\n",
    "3.The variation present in the PCs decrease as we move from the 1st PC to the last one, hence the importance.\n",
    "\n",
    "The least important PCs are also sometimes useful in regression, outlier detection, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Clustering is the task of dividing the population or data points into a number of groups such that data points in the same groups are more similar to other data points in the same group than those in other groups. The aim is to segregate groups with similar traits and assign them into clusters.\n",
    "\n",
    "or \n",
    "\n",
    "Clustering is a technique that groups similar data points such that the points in the same group are more similar to each other than the points in the other groups. The group of similar data points is called a Cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Hierarchical clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Hierarchical clustering, also known as hierarchical cluster analysis, is an algorithm that groups similar objects into groups called clusters. The endpoint is a set of clusters, where each cluster is distinct from each other cluster, and the objects within each cluster are broadly similar to each other.\n",
    "This clustering technique is divided into two types:\n",
    "1.Agglomerative\n",
    "2.Divisive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.1. Agglomerative Hierarchical clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Agglomerative Hierarchical clustering Technique: In this technique, initially each data point is considered as an individual cluster. At each iteration, the similar clusters merge with other clusters until one cluster or K clusters are formed.\n",
    "\n",
    "The basic algorithm of Agglomerative is straight forward.\n",
    "•Compute the proximity matrix\n",
    "•Let each data point be a cluster\n",
    "•Repeat: Merge the two closest clusters and update the proximity matrix\n",
    "•Until only a single cluster remains"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1.2. Divisive Hierarchical clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Divisive Hierarchical clustering Technique: Divisive Hierarchical clustering is exactly the opposite of the Agglomerative Hierarchical clustering. In Divisive Hierarchical clustering, we consider all the data points as a single cluster and in each iteration, we separate the data points from the cluster which are not similar. Each data point which is separated is considered as an individual cluster. In the end, we’ll be left with n clusters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Calculating the similarity between two clusters is important to merge or divide the clusters. There are certain approaches which are used to calculate the similarity between two clusters:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•MIN: Also known as single linkage algorithm can be defined as the similarity of two clusters C1 and C2 is equal to the minimum of the similarity between points Pi and Pj such that Pi belongs to C1 and Pj belongs to C2. Mathematically this can be written as,\n",
    "Sim(C1,C2) = Min Sim(Pi,Pj) such that Pi ∈ C1 & Pj ∈ C2\n",
    "In simple words, pick the two closest points such that one point lies in cluster one and the other point lies in cluster 2 and take their similarity and declare it as the similarity between two clusters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•MAX: Also known as the complete linkage algorithm, this is exactly opposite to the MIN approach. The similarity of two clusters C1 and C2 is equal to the maximum of the similarity between points Pi and Pj such that Pi belongs to C1 and Pj belongs to C2. Mathematically this can be written as,\n",
    "Sim(C1,C2) = Max Sim(Pi,Pj) such that Pi ∈ C1 & Pj ∈ C2\n",
    "In simple words, pick the two farthest points such that one point lies in cluster one and the other point lies in cluster 2 and take their similarity and declare it as the similarity between two clusters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•Group Average: Take all the pairs of points and compute their similarities and calculate the average of the similarities. Mathematically this can be written as,\n",
    "sim(C1,C2) = ∑ sim(Pi, Pj)/|C1|*|C2|   where, Pi ∈ C1 & Pj ∈ C2"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•Distance between centroids: Compute the centroids of two clusters C1 & C2 and take the similarity between the two centroids as the similarity between two clusters."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "•Ward’s Method:This approach of calculating the similarity between two clusters is exactly the same as Group Average except that Ward’s method calculates the sum of the square of the distances Pi and PJ. Mathematically this can be written as,\n",
    "sim(C1,C2) = ∑ (dist(Pi, Pj))²/|C1|*|C2|"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The Hierarchical clustering Technique can be visualized using a Dendrogram.\n",
    "A Dendrogram is a tree-like diagram that records the sequences of merges or splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Limitations of Hierarchical clustering Technique:\n",
    "1.There is no mathematical objective for Hierarchical clustering.\n",
    "2.All the approaches to calculate the similarity between clusters has its own disadvantages.\n",
    "3.High space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. K-means clustering"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "k-means is  one of  the simplest unsupervised  learning  algorithms  that  solve  the well  known clustering problem. The procedure follows a simple and  easy  way  to classify a given data set  through a certain number of  clusters (assume k clusters) fixed apriori. The  main  idea  is to define k centers, one for each cluster. These centers  should  be placed in a cunning  way  because of  different  location  causes different  result. So, the better  choice  is  to place them  as  much as possible  far away from each other. The  next  step is to take each point belonging  to a  given data set and associate it to the nearest center. When no point  is  pending,  the first step is completed and an early groups are formed. At this point we need to re-calculate k new centroids as barycenter of  the clusters resulting from the previous step. After we have these k new centroids, a new binding has to be done  between  the same data set points  and  the nearest new center. A loop has been generated. As a result of  this loop we  may  notice that the k centers change their location step by step until no more changes  are done or  in  other words centers do not move any more. Finally, this  algorithm  aims at  minimizing  an objective function know as squared error function:  \n",
    "\n",
    "Algorithmic steps for k-means clustering:\n",
    "\n",
    "Let  X = {x1,x2,x3,……..,xn} be the set of data points and V = {v1,v2,…….,vc} be the set of centers.\n",
    "\n",
    "1) Randomly select ‘c’ cluster centers.\n",
    "2) Calculate the distance between each data point and cluster centers.\n",
    "3) Assign the data point to the cluster center whose distance from the cluster center is minimum of all the cluster centers..\n",
    "4) Recalculate the new cluster center.  \n",
    "5) Recalculate the distance between each data point and new obtained cluster centers.\n",
    "6) If no data point was reassigned then stop, otherwise repeat from step 3).\n",
    "\n",
    " \n",
    "\n",
    "Advantages:\n",
    "\n",
    "1) Fast, robust and easier to understand.\n",
    "2) Relatively efficient: O(tknd), where n is number of objects, k is number of number of clusters, d is number of dimension of each object, and t is number of iterations. Normally, k, t, d << n.\n",
    "3) Gives best result when data set are distinct or well separated from each other.\n",
    "\n",
    "Disadvantages:\n",
    "\n",
    "1) The learning algorithm requires apriori specification of the number of  cluster centers. \n",
    "2) If there are two highly overlapping data then k-means will not be able to resolve.\n",
    "3) The learning algorithm is not invariant to non-linear transformations i.e. with different representation of data we get\n",
    "different results (data represented in form of cartesian co-ordinates and polar co-ordinates will give different results).\n",
    "4) Euclidean distance measures can unequally weight underlying factors. \n",
    "5) The learning algorithm provides the local optima of the squared error function. \n",
    "6) Randomly choosing of the cluster center cannot lead us to the fruitful result. Pl. refer Fig.\n",
    "7) Applicable only when mean is defined i.e. fails for categorical data.\n",
    "8) Unable to handle noisy data and outliers. \n",
    "9) Algorithm fails for non-linear data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Gaussian mixture models"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
