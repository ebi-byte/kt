{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is a measure quantifying the likelihood that events will occur. A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results is assigned a value of one. The opposite or complement of an event A is the event not A and its probability is given by P(not A) = 1 − P(A).\n",
    "\n",
    "Experiment – are the uncertain situations, which could have multiple outcomes. Whether it rains on a daily basis is an experiment.\n",
    "Outcome is the result of a single trial. So, if it rains today, the outcome of today’s trial from the experiment is “It rained”.\n",
    "Event is one or more outcome from an experiment. “It rained” is one of the possible event for this experiment.\n",
    "Probability is a measure of how likely an event is. So, if it is 60% chance that it will rain tomorrow, the probability of Outcome “it rained” for tomorrow is 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Independent Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two events, A and B are independent then the joint probability is\n",
    "P(A and B) = P(A ∩ B) = P(A).P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Mutually Exclusive Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If either event A or event B but never both occurs on a single performance of an experiment, then they are called mutually exclusive events. \n",
    "If two events are mutually exclusive then the probability of both occurring is denoted as \n",
    "P ( A ∩ B ) which is equal to 0.\n",
    "If two events are mutually exclusive then the probability of either occurring is denoted as \n",
    "P ( A ∪ B ) which is equal to P(A) + P(B) − P(A ∩ B) = P(A) + P(B) − 0 = P(A) + P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written as P(A ∣ B) and is defined as \n",
    "P(A ∣ B) = P(A ∩ B) / P(B). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes’ theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes’ theorem, a person's age can be used to more accurately assess the probability that they have cancer than can be done without knowledge of the person’s age. \n",
    "Bayes’ theorem is stated mathematically as the following equation:\n",
    "P(A ∣ B) = (P(B ∣ A). P(A)) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. A probability distribution is specified in terms of an underlying sample space, which is the set of all possible outcomes of the random phenomenon being observed. The sample space may be the set of real numbers or a set of vectors, or it may be a list of non-numerical values; for example, the sample space of a coin flip would be {heads, tails}. \n",
    "\n",
    "Probability distributions are generally divided into two classes - Discrete and Continuous. A discrete probability distribution (applicable to the scenarios where the set of possible outcomes is discrete) can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function. Well-known discrete probability distributions used in statistical modeling include the Poisson, the Bernoulli, the binomialand the geometric distribution. On the other hand, a continuous probability distribution (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range) is typically described by probability density functions. There are many examples of continuous probability distributions: normal, uniform, chi-squared distribution.\n",
    "\n",
    "A probability distribution whose sample space is one-dimensional is called univariate, while a distribution whose sample space is a vector space of dimension 2 or more is called multivariate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. Python"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel supports a wide range of distribution functions which are mostly used to obtain p-values(INV) or critical values(DIST).\n",
    "\n",
    "#### Normal Distribution: \n",
    "It is a bell shaped curve with tails on both sides. Normal distribution is characterised by two parameters - mean and standard deviation. The special case when mean = 0, standard deviation = 1 is known as standard normal distribution. \n",
    "\n",
    "In Excel, NORMDIST directly gives the cumulative distribution function i.e. Pr(X <= x). It follows the syntax NORMDIST(X,mean,standard dev.,cumulative) where X is normally distributed and cumulative is a switch. If set to TRUE, this switch tells Excel to calculate the probability of a variable being less than or equal to x; if set to FALSE, it tells Excel to calculate the probability of a variable being exactly equal to x.\n",
    "\n",
    "In Python, normal distributions can be generated using the numpy library. The syntax is numpy.random.normal(loc= mean, scale= std.deviation, size= 30).\n",
    "\n",
    "The following is the probability density function for various set of parameters\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/commons/7/74/Normal_Distribution_PDF.svg)\n",
    "The red curve corresponds to the standard normal distribution.\n",
    "\n",
    "#### T Distribution: \n",
    "T distribution is mainy used in estimating the mean of a large population whose standard deviation is unknown. We take a sample with n elements and use its mean and standard deviation in estimating the population mean. The T-distribution is also used for various testing scenarios which will be discussed in the hypothesis testing section. The degrees of freedom = n-1, where n is the number of elements in the sample.\n",
    "\n",
    "In excel, TDIST gives the probability of being in the right tail i.e. Pr(X > x), or of being in both tails i.e. Pr(|X| > x). THe syntax is TDIST(X,degrees of freedom,tails) where tails specify the number of distribution tails to return, one-tailed distribution: 1, two-tailed distribution: 2.\n",
    "\n",
    "In python, the syntax to create a random variable with T- distribution is np.random.standard_t(10, size=100000) where 10 is the number of degrees of freedom.\n",
    "\n",
    "The following is the T-table which gives the T-value for various degrees of freedom.\n",
    "![alt text](http://isoconsultantpune.com/wp-content/uploads/2015/06/t-distribution-table.png)\n",
    "\n",
    "#### Poisson Distribution: \n",
    "It is a discrete probability distribution that expresses the probability of a given number of events occurring in a fixed interval of time or space if these events occur with a known constant rate and independently of the time since the last event. The Poisson distribution can also be used for the number of events in other specified intervals such as distance, area or volume. For instance, an individual keeping track of the amount of mail they receive each day may notice that they receive an average number of 4 letters per day. If receiving any particular piece of mail does not affect the arrival times of future pieces of mail, i.e., if pieces of mail from a wide range of sources arrive independently of one another, then a reasonable assumption is that the number of pieces of mail received in a day obeys a Poisson distribution.\n",
    "\n",
    "The Excel POISSON function calculates the Poisson Probability Mass Function or the Cumulative Poisson Probability Function for a supplied set of parameters. The syntax of the function is: POISSON( x, mean, cumulative ).\n",
    "\n",
    "In python, the syntax to create a random variable with Poisson distribution is np.random.poisson(lam=1.0, size=100) where lam is the number of events occuring in the given fixed interval of time.\n",
    "\n",
    "![alt text](http://work.thaslwanter.at/Stats/html/_images/Poisson_distribution_pmf.png)\n",
    "\n",
    "#### Chi-Squared Distribution: \n",
    "The chi-squared distribution is commonly used to study variation in the percentage of something across samples, such as the fraction of the day people spend watching television. It is used to determine how closely the observed data fit the expected data. Chi-Square goodness of fit test determines how well theoretical distribution (such as normal, binomial, or Poisson) fits the empirical distribution. In Chi-Square goodness of fit test, sample data is divided into intervals. Then the numbers of points that fall into the interval are compared, with the expected numbers of points in each interval.\n",
    "\n",
    "Another use of Chi-Squared Distribution is for the test of independence. A chi-squared test for independence, tests if there is a significant relationship between two or more groups of categorical data from the same population. The null hypothesis for this test is that there is no relation. It is one of the most commonly used tests in statistics. In order to use this test, your observations should be independent.\n",
    "\n",
    "The syntax in Excel is CHISQ.DIST(x,deg_freedom,cumulative) where degrees of freedom = no. of categories - 1.\n",
    "\n",
    "In python, the syntax to create a random variable with Chi-Squared distribution is numpy.random.chisquare(df, size= 100. This draws samples from a chi-square distribution with df independent random variables, each with standard normal distributions (mean 0, variance 1).\n",
    "\n",
    "![alt text](http://labx.iasfbo.inaf.it/2014/resources/Chi-square_Distribution.jpg)\n",
    "\n",
    "\n",
    "#### Binomial Distribution: \n",
    "When you have a limited number of independent trials, or tests, which can either succeed or fail and when success or failure of any one trial is independent of other trials, the probability distribution is represented by binomial distribution. \n",
    "\n",
    "The syntax is BINOM.DIST(number_s,trials,probability_s,cumulative) where number_s is the specified number of successes that we want, trials equals the number of trials we’ll look at, probability_s equals the probability of success in a trial. \n",
    "\n",
    "Examples of binomial distribution problems: The number of defective/non-defective products in a production run. Yes/No Survey (such as asking 150 people if they watch ABC news). Vote counts for a candidate in an election. The number of successful sales calls. The number of male/female workers in a company.\n",
    "\n",
    "In python, the syntax to create a random variable with Binomial distribution is numpy.random.binomial(n,p,size=100) where n is the number of trials, p is the probability of success in a trail.\n",
    "\n",
    "![alt text](https://www.boost.org/doc/libs/1_56_0/libs/math/doc/graphs/binomial_pdf_1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing involves the careful construction of two statements: the null hypothesis and the alternative hypothesis. The aim of hypothesis testing is to find out if there is sufficient evidence to reject the null hypothesis in favour of alternate hypothesis. If there is not sufficient evidence, then we don't reject the null hypothesis. \n",
    "\n",
    "Null hypothesis: “x is equal to y.” Alternative hypothesis: “x is not equal to y.”\n",
    "\n",
    "Null hypothesis: “x is at least y.” Alternative hypothesis: “x is less than y.”\n",
    "\n",
    "Null hypothesis: “x is at most y.” Alternative hypothesis: “x is greater than y.”"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis reflects that there will be no observed effect in our experiment. This hypothesis is denoted by Ho. The null hypothesis is what we attempt to find evidence against in our hypothesis test. We hope to obtain a small enough p-value that it is lower than our level of significance alpha and we are justified in rejecting the null hypothesis. If our p-value is greater than alpha, then we fail to reject the null hypothesis. \n",
    "\n",
    "For example, if we are studying a new treatment, the null hypothesis is that our treatment will not change our subjects in any meaningful way. In other words, the treatment will not produce any effect in our subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Alternate Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative or experimental hypothesis reflects that there will be an observed effect for our experiment. This hypothesis is denoted by H1. The alternative hypothesis is what we are attempting to demonstrate in an indirect way by the use of our hypothesis test. If the null hypothesis is rejected, then we accept the alternative hypothesis. \n",
    "\n",
    "If the null hypothesis is not rejected, then we do not accept the alternative hypothesis. If we are studying a new treatment, then the alternative hypothesis is that our treatment does, in fact, change our subjects in a meaningful and measurable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Comparing sample mean with population mean (T - Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is especially useful to compare a mean from a random sample to an established data source such as census data to ensure you have an unbiased sample. We will understand this with the help of an example. A random sample of 30 incoming college freshmen revealed the following statistics: mean age 19.5 years, standard deviation 1 year. The college database shows the mean age for previous incoming students was 18. Assumptions for comparing the sample and population mean: Random sampling, normal distribution. \n",
    "\n",
    "Let's first declare the hypotheses. \n",
    "\n",
    "Ho: There is no significant difference between the mean age of past college students and the mean age of current incoming college students. \n",
    "\n",
    "H1: There is a significant difference between the mean age of past college students and the mean age of current incoming college students.\n",
    "\n",
    "The second step is to set the rejection criteria. Significance level .05 alpha, 2-tailed test, Degrees of Freedom = n-1 or 29, Critical value from t-distribution = 2.045.\n",
    "\n",
    "Now, we will compute the test statistic and compare it with the critical value to decide whether to accept or reject the null hypothesis in favour of the alternate hypothesis.\n",
    "\n",
    "$$ Sx = S/\\sqrt{n} $$\n",
    "$$ Sx = 1/\\sqrt{30} $$\n",
    "$$ Sx = 0.183 $$\n",
    "\n",
    "Test statistic: \n",
    "$$ t = (sample mean - population mean) / Sx $$\n",
    "$$ t = (19.5 - 18)/ 0.183 $$\n",
    "$$ t = 8.817 $$\n",
    "\n",
    "Given that the test statistic (8.197) exceeds the critical value (2.045), the null hypothesis is rejected in favor of the alternative. There is a statistically significant difference between the mean age of the current class of incoming students and the mean age of freshman students from past years. If the results had not been significant, the null hypothesis would not have been rejected. This would be interpreted as the following: There is insufficient evidence to conclude there is a statistically significant difference in the ages of current and past freshman students.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=8.828807651463716, pvalue=1.02892601115938e-09)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "np.random.seed(6) # fixing the random seed\n",
    "\n",
    "incomers_ages = np.random.normal(loc= 19.5, scale= 1, size= 30) #sample of size 30 with mean=19.5 and std. dev=1\n",
    "\n",
    "stats.ttest_1samp(a= incomers_ages, popmean= 18) # T-test for comparing the sample mean and the pop. mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as we can observe that the p-value is less than 0.05, there is sufficient evidence to reject the null hypothesis being that the population mean= sample mean. The conclusion is that the sample mean is diiferent from the population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Comparing two sample means (Two sample T - Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand this concept with the help of an example. An experiment is conducted to determine whether intensive tutoring (covering a great deal of material in a fixed amount of time) is more effective than paced tutoring (covering less material in the same amount of time). Two randomly chosen groups are tutored separately and then administered proficiency tests. Use a significance level of α < 0.05, single tail.\n",
    "\n",
    "Very different means can occur by chance if there is great variation among the individual samples. In order to account for the variation, we take the difference of the sample means and divide by the standard error in order to standardize the difference. Since, we do not know the population standard deviation, we estimate it using the two sample standard deviations from our independent samples.\n",
    "\n",
    "Null Hypothesis: mean1 = mean2; \n",
    "\n",
    "Alternate hypothesis: mean1 - mean2 > 0\n",
    "\n",
    "Data: Sample 1: n= 12, mean1 = 46.31, s1 = 6.44. Sample 2: n= 10, mean2 = 42.79, s2 = 7.52.\n",
    "\n",
    "$$ t = (mean1 - mean2)/\\sqrt( s1^2/n1^2 + s2^2/n2^2) $$\n",
    "\n",
    "The number of degrees of freedom for the problem is smaller of n1-1 or n2-1.\n",
    "\n",
    "With the given data, t is calculated to be 1.66. The degrees of freedom parameter is the smaller of (12 – 1) and (10 – 1), or 9. Because this is a one‐tailed test, the alpha level (0.05) is not divided by two. In the t-table, the critical value is found to be 1.833. The computed t of 1.166 does not exceed the tabled value, therefore the null hypothesis cannot be rejected. This test has not provided statistically significant evidence that intensive tutoring is superior to paced tutoring. \n",
    "\n",
    "Let's see an example with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=0.5110451757011513, pvalue=0.6149147980342374)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "np.random.seed(12)\n",
    "score_intensive = np.random.normal(loc= 46.31, scale= 6.44, size= 12) # sample for intensive tutoring \n",
    "score_paced = np.random.normal(loc= 42.79, scale= 7.52, size= 10) # sample for paced tutoring\n",
    "\n",
    "stats.ttest_ind(a= score_intensive, b= score_paced) # Two sample t test comparing the means of the two samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, p value is greater than 0.05 and therefore, the null hypothesis cannot be rejected. The conclusion is that there is no sufficient evidence to prove that the two sample means are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. P - Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test statistics are helpful, but it can be more helpful to assign a p-value to these statistics. \n",
    "\n",
    "A p-value is the probability that, if the null hypothesis were true, we would observe a statistic at least as extreme as the one observed. In simple terms, p-value is the probability of your null hypothesis actually being observed. For example, p=0.05 implies that one in a twenty observations follow your null hypothesis. \n",
    "\n",
    "Typically, before we conduct a hypothesis test, we choose a threshold value. If we have any p-value that is less than or equal to this threshold, then we reject the null hypothesis. Otherwise we fail to reject the null hypothesis. This threshold is called the level of significance of our hypothesis test, and is denoted by the Greek letter alpha. There is no value of alpha that always defines statistical significance. However, the most common used criteria is that P < 0.05 is statistically significant and P < 0.001 is statistically highly significant.  \n",
    "\n",
    "![alt text](https://upload.wikimedia.org/wikipedia/en/0/00/P-value_Graph.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Test Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to perform the hypothesis test. In general, a test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis. There is a wide range of test statistics used and we will look into a few of the important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z Value\n",
    "Simply put, a z-score is the number of standard deviations from the mean a data point is. The basic z score formula for a sample is: z = (x – μ) / σ, where μ is the mean of the population and σ is its standard deviation. If you pick any point in a normal distribution and desire to know its location in relevance to the mean of the population, z-score comes in handy. Z-score gives the point's location in terms of how many standard deviations away from the mean.\n",
    "\n",
    "Sample problem: In general, the mean height of women is 65″ with a standard deviation of 3.5″. What is the probability of finding a random sample of 50 women with a mean height of 70″, assuming the heights are normally distributed? \n",
    "\n",
    "z = (x – μ) / (σ / √n)\n",
    "= (70 – 65) / (3.5/√50) = 5 / 0.495 = 10.1\n",
    "\n",
    "We know that 99% of values fall within 3 standard deviations from the mean in a normal probability distribution. Therefore, there’s less than 1% probability that any sample of women will have a mean height of 70″.\n",
    "\n",
    "![alt text](https://access-excel.tips/wp-content/uploads/2015/09/z-score-02.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHI Square\n",
    "Chi Square is highly useful in comparing the observed results and expected results of an experiment. The chi-square value can be obtained with the formula $ chisquare=sum(i=1tok)((oi−ei)^2/ei) $\n",
    ", where o is the observed value, e is the expected value and i is i'th position in the contingency table. Lower the chi-square value, higher is the similarity between observed and expected phenomena. You could also take your calculated chi-square value and compare it to a critical value from a chi-square table. If the chi-square value is more than the critical value, then there is a significant difference.\n",
    "\n",
    "Consider a standard package of milk chocolate M&Ms. There are six different colors: red, orange, yellow, green, blue and brown. Suppose that we are curious about the distribution of these colors and ask, do all six colors occur in equal proportion? This is the type of question that can be answered with a goodness of fit test.\n",
    "\n",
    "The null and alternative hypotheses for our goodness of fit test reflect the assumption that we are making about the population. Since we are testing whether the colors occur in equal proportions, our null hypothesis will be that all colors occur in the same proportion. More formally, if p1 is the population proportion of red candies, p2 is the population proportion of orange candies, and so on, then the null hypothesis is that p1 = p2 = . . . = p6 = 1/6. The alternative hypothesis is that at least one of the population proportions is not equal to 1/6.\n",
    "\n",
    "The actual counts are the number of candies for each of the six colors. The expected count refers to what we would expect if the null hypothesis were true. We will let n be the size of our sample. The expected number of red candies is p1 n or n/6. In fact, for this example, the expected number of candies for each of the six colors is simply n times pi, or n/6.\n",
    "\n",
    "We will now calculate a chi-square statistic for a specific example. Suppose that we have a simple random sample of 600 M&M candies with the following distribution:\n",
    "\n",
    "212 of the candies are blue.\n",
    "147 of the candies are orange.\n",
    "103 of the candies are green.\n",
    "50 of the candies are red.\n",
    "46 of the candies are yellow.\n",
    "42 of the candies are brown.\n",
    "\n",
    "If the null hypothesis were true, then the expected counts for each of these colors would be (1/6) x 600 = 100. We now use this in our calculation of the chi-square statistic.\n",
    "\n",
    "We calculate the contribution to our statistic from each of the colors. Each is of the form (Actual – Expected)^2/Expected.:\n",
    "\n",
    "For blue we have (212 – 100)^2/100 = 125.44\n",
    "\n",
    "For orange we have (147 – 100)^2/100 = 22.09\n",
    "\n",
    "For green we have (103 – 100)^2/100 = 0.09\n",
    "\n",
    "For red we have (50 – 100)^2/100 = 25\n",
    "\n",
    "For yellow we have (46 – 100)^2/100 = 29.16\n",
    "\n",
    "For brown we have (42 – 100)^2/100 = 33.64\n",
    "\n",
    "We then total all of these contributions and determine that our chi-square statistic is 125.44 + 22.09 + 0.09 + 25 +29.16 + 33.64 =235.42.\n",
    "\n",
    "Degrees of Freedom \n",
    "The number of degrees of freedom for a goodness of fit test is simply one less than the number of levels of our variable. Since there were six colors, we have 6 – 1 = 5 degrees of freedom.\n",
    "\n",
    "Chi-square Table and P-Value \n",
    "The chi-square statistic of 235.42 that we calculated corresponds to a particular location on a chi-square distribution with five degrees of freedom. We now need a p-value, to determines the probability of obtaining a test statistic at least as extreme as 235.42 while assuming that the null hypothesis is true.\n",
    "\n",
    "We find that our test statistic with five degrees of freedom has a p-value of 7.29 x 10-49. This is an extremely small p-value.\n",
    "\n",
    "Decision Rule \n",
    "We make our decision on whether to reject the null hypothesis based on the size of the p-value. Since we have a very miniscule p-value, we reject the null hypothesis. We conclude that M&Ms are not evenly distributed among the six different colors. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F Value\n",
    "F values are generally used to compare the variances between two samples. \n",
    "\n",
    "The F value is given by between-group variability/ within group variability. Between group variability = $ sum(i=1 to k)Ni((Yi - Y)^2/k-1) $, where Ni is the number of observations in the i'th group, k is the number of groups, Yi is the sample mean of the i'th group and Y is the overall mean. Within group variability = $ sum(i=1 to k)sum(j=1 to Ni)((Yij - Yi)^2/Ni-k) $, where Yij is the jth sample in the i'th group. \n",
    "\n",
    "The F-statistic follows the F-distribution with degrees of freedom d1 = k-1 and d2 = Ni-k. The F-statistic compares the effects of all the variables together whereas the T-statistic only considers one variable.\n",
    "\n",
    "F-value should always be used along with the p-value. If F-value is greater than the F-critical value and p-value is less than p-critical value then the null hypothesis can be rejected in favour of the alternate hypothesis. When there is a conflict, p-value gets more importance.\n",
    "\n",
    "Sample problem: Conduct a two tailed F Test on the following samples:\n",
    "Sample 1: Variance = 109.63, sample size = 41.\n",
    "Sample 2: Variance = 65.99, sample size = 21.\n",
    "\n",
    "Step 1: Write your hypothesis statements:\n",
    "    Ho: No difference in variances.\n",
    "    Ha: Difference in variances.\n",
    "\n",
    "Step 2: Calculate your F critical value. Put the highest variance as the numerator and the lowest variance as the denominator:\n",
    "F Statistic = variance 1/ variance 2 = 109.63 / 65.99 = 1.66\n",
    "\n",
    "Step 3: Calculate the degrees of freedom:\n",
    "The degrees of freedom in the table will be the sample size -1, so:\n",
    "    Sample 1 has 40 df (the numerator).\n",
    "    Sample 2 has 20 df (the denominator).\n",
    "\n",
    "Step 4: Choose an alpha level. No alpha was stated in the question, so use 0.05 (the standard “go to” in statistics). This needs to be halved for the two-tailed test, so use 0.025.\n",
    "\n",
    "Step 5: Find the critical F Value using the F Table. There are several tables, so make sure you look in the alpha = .025 table. Critical F (40,20) at alpha (0.025) = 2.287.\n",
    "\n",
    "Step 6: Compare your calculated value (Step 2) to your table value (Step 5). If your calculated value is higher than the table value, you can reject the null hypothesis:\n",
    "F calculated value: 1.66\n",
    "F value from table: 2.287.\n",
    "1.66 < 2 .287.\n",
    "So we cannot reject the null hypothesis.\n",
    "\n",
    "![alt text](http://www.statext.com/tables/F-Table(21-30,1-30,0.05).jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Type I and Type II Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion), while a type II error is the non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion). Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is treated as a statistical impossibility.\n",
    "\n",
    "The likelihood of type I error, is equal to the level of significance, that the researcher sets for his test. Here the level of significance refers to the chances of making type I error. Often, the significance level is set to 0.05 (5%), implying that it is acceptable to have a 5% probability of incorrectly rejecting the null hypothesis. The likelihood of making type II error is analogous to the power of the test. The power of the test is the probability that the test will find a statistically significant difference between men and women, as a function of the size of the true difference between those two populations. As the sample size increases, the power of test also increases, that results in the reduction in risk of making type II error. Power = 1 - risk of Type II error.\n",
    "\n",
    "Consider a sample problem.\n",
    "\n",
    "Suppose the mean weight of King Penguins found in an Antarctic colony last year was 15.4 kg. Assume in a random sample 35 penguins, the standard deviation of the weight is 2.5 kg. If actual mean penguin weight is 15.1 kg, what is the probability of type II error for a hypothesis test at .05 significance level? \n",
    "\n",
    "We begin with computing the standard error estimate, SE. \n",
    "\n",
    "n = 35                # sample size \n",
    "\n",
    "s = 2.5               # sample standard deviation \n",
    "\n",
    "SE = s/sqrt(n); SE    # standard error estimate \n",
    "\n",
    "0.42258 \n",
    "\n",
    "We next compute the lower and upper bounds of sample means for which the null hypothesis μ = 15.4 would not be rejected. \n",
    "\n",
    "alpha = .05           # significance level \n",
    "\n",
    "mu0 = 15.4            # hypothetical mean \n",
    "\n",
    "I = c(alpha/2, 1-alpha/2) \n",
    "\n",
    "q = mu0 + qt(I, df=n-1) * SE; q \n",
    "\n",
    "14.541 16.259 \n",
    "\n",
    "Therefore, so long as the sample mean is between 14.541 and 16.259 in a hypothesis test, the null hypothesis will not be rejected. Since we assume that the actual population mean is 15.1, we can compute the lower tail probabilities of both end points. \n",
    "\n",
    "mu = 15.1             # assumed actual mean \n",
    "\n",
    "p = pt((q - mu)/SE, df=n-1); p \n",
    "\n",
    "0.097445 0.995168 \n",
    "\n",
    "Finally, the probability of type II error is the probability between the two end points. \n",
    "\n",
    "diff(p)               # p2-p1 \n",
    "0.89772 \n",
    "\n",
    "Thus, if the penguin sample size is 35, the sample standard deviation of penguin weight is 2.5 kg and the actual mean population weight is 15.1 kg, then the probability of type II error for testing the null hypothesis μ = 15.4 at .05 significance level is 89.8%, and the power of the hypothesis test is 10.2%. \n",
    "\n",
    "![alt text](https://www.oreilly.com/library/view/mastering-python-for/9781784390150/graphics/B03450_02_22.jpg)\n",
    "\n",
    "A Confusion matrix, also known as an error matrix, is a specific table layout that allows visualization of the performance of an algorithm. It is a special kind of contingency table, with two dimensions (\"actual\" and \"predicted\"), and identical sets of \"classes\" in both dimensions (each combination of dimension and class is a variable in the contingency table). \n",
    "\n",
    "![alt text](https://www.dataschool.io/content/images/2015/01/confusion_matrix2.png)\n",
    "\n",
    "There are two possible predicted classes: \"yes\" and \"no\". If we were predicting the presence of a disease, for example, \"yes\" would mean they have the disease, and \"no\" would mean they don't have the disease.\n",
    "\n",
    "The classifier made a total of 165 predictions (e.g., 165 patients were being tested for the presence of that disease).\n",
    "\n",
    "Out of those 165 cases, the classifier predicted \"yes\" 110 times, and \"no\" 55 times.\n",
    "In reality, 105 patients in the sample have the disease, and 60 patients do not.\n",
    "\n",
    "Let's now define the most basic terms, which are whole numbers (not rates):\n",
    "\n",
    "True positives (TP): These are cases in which we predicted yes (they have the disease), and they do have the disease.\n",
    "    \n",
    "True negatives (TN): We predicted no, and they don't have the disease.\n",
    "\n",
    "False positives (FP): We predicted yes, but they don't actually have the disease. (Also known as a \"Type I error.\")\n",
    "    \n",
    "False negatives (FN): We predicted no, but they actually do have the disease. (Also known as a \"Type II error.\")\n",
    "\n",
    "Accuracy: Overall, how often is the classifier correct? \n",
    "(TP+TN)/total = (100+50)/165 = 0.91\n",
    "\n",
    "Misclassification Rate: Overall, how often is it wrong? \n",
    "(FP+FN)/total = (10+5)/165 = 0.09\n",
    "equivalent to 1 minus Accuracy\n",
    "also known as \"Error Rate\"\n",
    "\n",
    "True Positive Rate: When it's actually yes, how often does it predict yes? \n",
    "TP/actual yes = 100/105 = 0.95\n",
    "also known as \"Sensitivity\" or \"Recall\"\n",
    "\n",
    "False Positive Rate: When it's actually no, how often does it predict yes? \n",
    "FP/actual no = 10/60 = 0.17\n",
    "\n",
    "True Negative Rate: When it's actually no, how often does it predict no? \n",
    "TN/actual no = 50/60 = 0.83\n",
    "equivalent to 1 minus False Positive Rate\n",
    "also known as \"Specificity\"\n",
    "\n",
    "Precision: When it predicts yes, how often is it correct? \n",
    "TP/predicted yes = 100/110 = 0.91\n",
    "\n",
    "Prevalence: How often does the yes condition actually occur in our sample? \n",
    "actual yes/total = 105/165 = 0.64\n",
    "\n",
    "F Score: This is a weighted average of the true positive rate (recall) and precision. The F1 score is the harmonic mean of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0. \n",
    "F Score = 2*(P.R/(P+R)) where P is precision and R is recall.\n",
    "\n",
    "##### ROC and AUC\n",
    "An ROC curve is the most commonly used way to visualize the performance of a binary classifier, and AUC is one of the best way to summarize its performance in a single number. \n",
    "\n",
    "AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. ROC is a probability curve and AUC represents degree or measure of separability. It tells how much model is capable of distinguishing between classes. Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.\n",
    "\n",
    "The ROC curve is plotted with TPR against the FPR where TPR is on y-axis and FPR is on the x-axis.\n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*pk05QGzoWhCgRiiFbz-oKQ.png)\n",
    "\n",
    "AUC stands for \"Area under the ROC Curve.\" That is, AUC measures the entire two-dimensional area underneath the entire ROC curve.\n",
    "\n",
    "An excellent model has AUC near to the 1 which means it has good measure of separability. A poor model has AUC near to the 0 which means it has worst measure of separability. In fact it means it is reciprocating the result. It is predicting 0s as 1s and 1s as 0s. And when AUC is 0.5, it means model has no class separation capacity whatsoever.\n",
    "\n",
    "AUC is desirable for the following two reasons:\n",
    "\n",
    "    AUC is scale-invariant. It measures how well predictions are ranked, rather than their absolute values.\n",
    "    AUC is classification-threshold-invariant. It measures the quality of the model's predictions irrespective of what classification threshold is chosen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.8. KS Stat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.8.1. One sample KS Test:\n",
    "\n",
    "The Kolmogorov-Smirnov Goodness of Fit Test (K-S test) compares your data with a known distribution and lets you know if they have the same distribution. Although the test is nonparametric — it doesn’t assume any particular underlying distribution — it is commonly used as a test for normality to see if your data is normally distributed.It’s also used to check the assumption of normality in Analysis of Variance. More specifically, the test compares a known hypothetical probability distribution (e.g. the normal distribution) to the distribution generated by your data — the empirical distribution function.\n",
    "\n",
    "The hypotheses for the test are:\n",
    "\n",
    "Null hypothesis (H0): the data comes from the specified distribution.\n",
    "\n",
    "Alternate Hypothesis (H1): at least one value does not match the specified distribution.\n",
    "\n",
    "That is,\n",
    "H0: P = P0, H1: P ≠ P0.\n",
    "Where P is the distribution of your sample (i.e. the EDF) and P0 is a specified distribution.\n",
    "\n",
    "Example:\n",
    "\n",
    "Step 1: Create an EDF for your sample data.\n",
    "\n",
    "Step 2: Specify the parent distribution. In this example, we will compare the emperical distribution with the gamma function.\n",
    "\n",
    "Step 3: Graph the functions together. A snapshot of the scatter graph might like this:\n",
    "\n",
    "![alt text](https://www.statisticshowto.datasciencecentral.com/wp-content/uploads/2016/07/ks-test.png)\n",
    "\n",
    "Step 4: Measure the greatest vertical distance. Let’s assume that the graph is of the entire sample and the largest vertical distance separating the two graphs is .04 (in the yellow highlighted box).\n",
    "\n",
    "Step 5: Look up the critical value in the K-S table value. The sample has 50 observations. At an alpha level of .05, the K-S table value is .190. \n",
    "\n",
    "![alt text](https://i0.wp.com/www.real-statistics.com/wp-content/uploads/2012/11/one-sample-ks-table.png)\n",
    "\n",
    "Step 6: Compare the results from Step 4 and Step 5. Since .04 is less than .190, the null hypothesis (that the distributions are the same) is accepted.\n",
    "\n",
    "In python, KS one sample test can be done using scipy.stats library. The syntax is\n",
    "\n",
    "    from scipy.stats import kstest\n",
    "    test_stat = kstest(x, 'norm')\n",
    "where x is your test sample and we are comparing it with the normal distribution.\n",
    "\n",
    "##### Advantages and Disadvantages\n",
    "Advantages include:\n",
    "\n",
    "    The test is distribution free. That means you don’t have to know the underlying population distribution for your data before running this test.\n",
    "    The D statistic (not to be confused with Cohen’s D) used for the test is easy to calculate.\n",
    "    It can be used as a goodness of fit test following regression analysis.\n",
    "    There are no restrictions on sample size; Small samples are acceptable.\n",
    "    Tables are readily available.\n",
    "\n",
    "Although the K-S test has many advantages, it also has a few limitations:\n",
    "\n",
    "    In order for the test to work, you must specify the location, scale, and shape parameters. If these parameters are estimated from the data, it invalidates the test. If you don’t know these parameters, you may want to run a less formal test (like the one outlined in the empirical distribution function article).\n",
    "    It generally can’t be used for discrete distributions, especially if you are using software (most software packages don’t have the necessary extensions for discrete K-S Test and the manual calculations are convoluted).\n",
    "    Sensitivity is higher at the center of the distribution and lower at the tails.\n",
    "    \n",
    "Two sample KS test compares whether the two samples given as input are from the same distribution or not. The syntax is \n",
    "\n",
    "    from scipy.stats import ks_2samp\n",
    "    test_stat = ks_2samp(x, y)\n",
    "where x and y are the 2 samples to be compared.  \n",
    "  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cross Validation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cross-validation, sometimes called rotation estimation or out-of-sample testing, is any of various similar model validation techniques for assessing how the results of a statistical analysis will generalize to an independent data set. It is mainly used in settings where the goal is prediction, and one wants to estimate how accurately a predictive model will perform in practice. In a prediction problem, a model is usually given a dataset of known data on which training is run (training dataset), and a dataset of unknown data (or first seen data) against which the model is tested (called the validation dataset or testing set). The goal of cross-validation is to test the model's ability to predict new data that was not used in estimating it, in order to flag problems like overfitting or selection bias and to give an insight on how the model will generalize to an independent dataset (i.e., an unknown dataset, for instance from a real problem). \n",
    "\n",
    "One round of cross-validation involves partitioning a sample of data into complementary subsets, performing the analysis on one subset (called the training set), and validating the analysis on the other subset (called the validation set or testing set). \n",
    "\n",
    "To reduce variability, in most methods multiple rounds of cross-validation are performed using different partitions, and the validation results are combined (e.g. averaged) over the rounds to give an estimate of the model's predictive performance. \n",
    "In summary, cross-validation combines (averages) measures of fitness in prediction to derive a more accurate estimate of model prediction performance.\n",
    "\n",
    "##### Types\n",
    "\n",
    "Two types of cross-validation can be distinguished: exhaustive and non-exhaustive cross-validation. \n",
    "\n",
    "##### Exhaustive cross-validation\n",
    "Exhaustive cross-validation methods are cross-validation methods which learn and test on all possible ways to divide the original sample into a training and a validation set. \n",
    "\n",
    "##### Leave-p-out cross-validation\n",
    "Leave-p-out cross-validation (LpO CV) involves using p observations as the validation set and the remaining observations as the training set. This is repeated on all ways to cut the original sample on a validation set of p observations and a training set.\n",
    "LpO cross-validation requires training and validating the model \n",
    "nCp times, where n is the number of observations in the original sample, and where p is the binomial coefficient. For p > 1 and for even moderately large n, LpO CV can become computationally infeasible. For example, with n = 100 and p = 30 = 30 percent of 100, 100C30  ≈ 3 × 10^25.\n",
    " \n",
    "##### Leave-one-out cross-validation\n",
    "Leave-one-out cross-validation (LOOCV) is a particular case of leave-p-out cross-validation with p = 1. \n",
    "The process looks similar to jackknife; however, with cross-validation one computes a statistic on the left-out sample(s), while with jackknifing one computes a statistic from the kept samples only. \n",
    "LOO cross-validation requires less computation time than LpO cross-validation because there are only \n",
    "nC1 passes rather than nCk. However, n passes may still require quite a large computation time, in which case other approaches such as k-fold cross validation may be more appropriate.\n",
    "\n",
    "##### Non-exhaustive cross-validation\n",
    "Non-exhaustive cross validation methods do not compute all ways of splitting the original sample. Those methods are approximations of leave-p-out cross-validation. \n",
    "\n",
    "##### k-fold cross-validation\n",
    "In k-fold cross-validation, the original sample is randomly partitioned into k equal sized subsamples. Of the k subsamples, a single subsample is retained as the validation data for testing the model, and the remaining k − 1 subsamples are used as training data. The cross-validation process is then repeated k times, with each of the k subsamples used exactly once as the validation data. The k results can then be averaged to produce a single estimation. \n",
    "\n",
    "The advantage of this method over repeated random sub-sampling is that all observations are used for both training and validation, and each observation is used for validation exactly once. 10-fold cross-validation is commonly used, but in general k remains an unfixed parameter. For example, setting k = 2 results in 2-fold cross-validation. In 2-fold cross-validation, we randomly shuffle the dataset into two sets d0 and d1, so that both sets are equal size (this is usually implemented by shuffling the data array and then splitting it in two). We then train on d0 and validate on d1, followed by training on d1 and validating on d0. \n",
    "\n",
    "When k = n (the number of observations), the k-fold cross-validation is exactly the leave-one-out cross-validation.\n",
    "\n",
    "##### Repeated random sub-sampling validation\n",
    "This method, also known as Monte Carlo cross-validation, creates multiple random splits of the dataset into training and validation data. For each such split, the model is fit to the training data, and predictive accuracy is assessed using the validation data. The results are then averaged over the splits. The advantage of this method (over k-fold cross validation) is that the proportion of the training/validation split is not dependent on the number of iterations (folds). The disadvantage of this method is that some observations may never be selected in the validation subsample, whereas others may be selected more than once. In other words, validation subsets may overlap. This method also exhibits Monte Carlo variation, meaning that the results will vary if the analysis is repeated with different random splits. \n",
    "\n",
    "As the number of random splits approaches infinity, the result of repeated random sub-sampling validation tends towards that of leave-p-out cross-validation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. A couple has two children, of which at least one is a boy. If the probabilities of having a boy or a girl are both \n",
    "50%, what is the probability that the couple has two boys?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, We might reason as follows: “We know that one is a boy, so the only question is whether the other one is a boy, and the chances of that being the case are 50%. So, the answer must be 50%. But, it happens to be incorrect.\n",
    "\n",
    "Define two events, A and B as follows:\n",
    "\n",
    "A = Both children are boys.\n",
    "B = One of them is a boy.\n",
    "\n",
    "We have to find P(A|B)\n",
    "\n",
    "From Bayes' Theorem, \n",
    "\n",
    "P(A|B) = P(B|A)P(A)/P(B) = (1)(1/4)/(3/4) = 1/3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. Jackie wants to determine the probability of a Type I Error before she  performs hypothesis testing on her sample of 40 individuals. Prior to  testing, she decides on a significance level of 2%. What is the  probability of a Type I Error in this case?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rejecting the null hypothesis Ho when in fact Ho is the truth is known as Type 1 error. Type 1 errors depend on the selection of significance level. In this problem, a significance level of 2% is selected. Therefore, the probability of a Tpe 1 error happening = 0.02"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. A man was able to complete 3 files a day on an average. Find the probability that he can complete 5 files the next day.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following formula can be used for computation of Poisson probability:\n",
    "\n",
    "P (x, $\\mu$) = $\\frac{(e^{-\\mu}) (\\mu^x)}{x!}$\n",
    "\n",
    "Here, ‘x’ represents the actual number of occurring successes that are resulting from the Poisson experiment, the value is ‘e’ is 2.71828 approximately, '$\\mu$' is the average of the number of successes that are within a specified region.\n",
    "\n",
    "Here we know this is a Poisson experiment with following values given:\n",
    "$\\mu$= 3, average number of files completed a day\n",
    "\n",
    "x = 5, the number of files required to be completed next day\n",
    "\n",
    "And e = 2.71828 being a constant\n",
    "\n",
    "On substituting the values in the Poisson distribution formula mentioned above we get the Poisson probability in this case.\n",
    "\n",
    "We get,\n",
    "\n",
    "P(x,$\\mu$) = $\\frac{(e^{-\\mu}) (\\mu^x)}{x!}$\n",
    "\n",
    "P (5, 3) = $\\frac{(2.71828)^{-3} (3^5)}{5!}$\n",
    "\n",
    "= 0.1008 approximately.\n",
    "\n",
    "Hence the probability for the person to complete 5 files the next day is 0.1008 approximately.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. Consider the following situation. You are given a task of analysing the weights, heights and number of students who attend a class on a particular day. Which distribution will you use for the analysis and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to choose a distribution for study, we have to first understand the data and the target requirements. \n",
    "\n",
    "Normal distribution describes continuous data which have a symmetric distribution, with a characteristic 'bell' shape.  \n",
    "Binomial distribution describes the distribution of binary data from a finite sample. Thus it gives the probability of getting r events out of n trials.  \n",
    "Poisson distribution describes the distribution of binary data from an infinite sample. Thus it gives the probability of getting r events in a population.\n",
    "\n",
    "In this example, consider the weights and heights of the students. The weights and heights will more or less form a symmetric distribution, and we might be looking into the mean, median or mode of these features so as to understand the data. Normal distribution is a bell shaped curve characterised by two parameters mean and standard deviation. Thus, for the analysis of these two variables, normal distribution will be the best choice. \n",
    "\n",
    "Consider the attendance of a class on a particular day. We will be looking into the probabilities of different number of students attending the class on a particular day. The Poisson distribution is one such distribution whose area under the curve at a particular point gives the probabilities of that many events happening. Thus, Poisson distribution will be an apt choice for these kinds of analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. If a group of students are treated using a new technique \"Smart Class\" and you want to check the proportion of students who will successfully pass the exam conducted at the end. Which distribution will you use and why?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data which can take only a binary (0 or 1) response, such as fail or pass in an exam, follow the binomial distribution provided *the underlying population response rate does not change*. Under these conditions, binomial distribution will be of the best use. In this example, if we can assume that the student passing rate will be independent of the attempt, then we can use binomial distribution for the required analysis.  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
