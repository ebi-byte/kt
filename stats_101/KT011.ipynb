{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability is a measure quantifying the likelihood that events will occur. A probability is a way of assigning every event a value between zero and one, with the requirement that the event made up of all possible results is assigned a value of one. The opposite or complement of an event A is the event not A and its probability is given by P(not A) = 1 − P(A).\n",
    "\n",
    "Experiment – are the uncertain situations, which could have multiple outcomes. Whether it rains on a daily basis is an experiment.\n",
    "Outcome is the result of a single trial. So, if it rains today, the outcome of today’s trial from the experiment is “It rained”.\n",
    "Event is one or more outcome from an experiment. “It rained” is one of the possible event for this experiment.\n",
    "Probability is a measure of how likely an event is. So, if it is 60% chance that it will rain tomorrow, the probability of Outcome “it rained” for tomorrow is 0.6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1. Independent Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If two events, A and B are independent then the joint probability is\n",
    "P(A and B) = P(A ∩ B) = P(A).P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2. Mutually Exclusive Events"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If either event A or event B but never both occurs on a single performance of an experiment, then they are called mutually exclusive events. \n",
    "If two events are mutually exclusive then the probability of both occurring is denoted as \n",
    "P ( A ∩ B ) which is equal to 0.\n",
    "If two events are mutually exclusive then the probability of either occurring is denoted as \n",
    "P ( A ∪ B ) which is equal to P(A) + P(B) − P(A ∩ B) = P(A) + P(B) − 0 = P(A) + P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3. Conditional Probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conditional probability is the probability of some event A, given the occurrence of some other event B. Conditional probability is written as P(A ∣ B) and is defined as \n",
    "P(A ∣ B) = P(A ∩ B) / P(B). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4. Bayes' Theorem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bayes’ theorem describes the probability of an event, based on prior knowledge of conditions that might be related to the event. For example, if cancer is related to age, then, using Bayes’ theorem, a person's age can be used to more accurately assess the probability that they have cancer than can be done without knowledge of the person’s age. \n",
    "Bayes’ theorem is stated mathematically as the following equation:\n",
    "P(A ∣ B) = (P(B ∣ A). P(A)) / P(B)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Probability Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A probability distribution is a mathematical function that provides the probabilities of occurrence of different possible outcomes in an experiment. A probability distribution is specified in terms of an underlying sample space, which is the set of all possible outcomes of the random phenomenon being observed. The sample space may be the set of real numbers or a set of vectors, or it may be a list of non-numerical values; for example, the sample space of a coin flip would be {heads, tails}. \n",
    "\n",
    "Probability distributions are generally divided into two classes - Discrete and Continuous. A discrete probability distribution (applicable to the scenarios where the set of possible outcomes is discrete) can be encoded by a discrete list of the probabilities of the outcomes, known as a probability mass function. Well-known discrete probability distributions used in statistical modeling include the Poisson, the Bernoulli, the binomialand the geometric distribution. On the other hand, a continuous probability distribution (applicable to the scenarios where the set of possible outcomes can take on values in a continuous range) is typically described by probability density functions. There are many examples of continuous probability distributions: normal, uniform, chi-squared distribution.\n",
    "\n",
    "A probability distribution whose sample space is one-dimensional is called univariate, while a distribution whose sample space is a vector space of dimension 2 or more is called multivariate."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1. In Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excel supports a wide range of distribution functions which are mostly used to obtain p-values(INV) or critical values(DIST).\n",
    "\n",
    "#### Normal Distribution: \n",
    "It is a bell shaped curve with tails on both sides. Normal distribution is characterised by two parameters - mean and standard deviation. The special case when mean = 0, standard deviation = 1 is known as standard normal distribution. In Excel, NORMDIST directly gives the cumulative distribution function i.e. Pr(X <= x). It follows the syntax NORMDIST(X,mean,standard dev.,cumulative) where X is normally distributed and cumulative is a switch. If set to TRUE, this switch tells Excel to calculate the probability of a variable being less than or equal to x; if set to FALSE, it tells Excel to calculate the probability of a variable being exactly equal to x.\n",
    "\n",
    "#### T Distribution: \n",
    "T distribution is mainy used in estimating the mean of a large population whose standard deviation is unknown. We take a sample with n elements and use its mean and standard deviation in estimating the population mean. The T-distribution is also used for various testing scenarios which will be discussed in the hypothesis testing section. The degrees of freedom = n-1, where n is the number of elements in the sample.In excel, TDIST gives the probability of being in the right tail i.e. Pr(X > x), or of being in both tails i.e. Pr(|X| > x). THe syntax is TDIST(X,degrees of freedom,tails) where tails specify the number of distribution tails to return, one-tailed distribution: 1, two-tailed distribution: 2.\n",
    "\n",
    "#### F Distribution: \n",
    "F-distributions are probability distributions in Excel that compare the ratio in variances of samples drawn from different populations. In Excel, the F.DIST function returns the left-tailed probability of observing a ratio of two samples’ variances as large as a specified f-value. The function takes the syntax F.DIST(x,deg_freedom1,deg_freedom2,cumulative) where x is specified f-value that you want to test; deg_freedom1 is the degrees of freedom in the first sample; deg_freedom2 is the degrees of freedom in the second sample.\n",
    "\n",
    "#### Chi-Squared Distribution: \n",
    "The chi-squared distribution is commonly used to study variation in the percentage of something across samples, such as the fraction of the day people spend watching television. It is used to determine how closely the observed data fit the expected data. The syntax is CHISQ.DIST(x,deg_freedom,cumulative) where degrees of freedom = no. of categories - 1.\n",
    "\n",
    "#### Binomial Distribution: \n",
    "When you have a limited number of independent trials, or tests, which can either succeed or fail and when success or failure of any one trial is independent of other trials, the probability distribution is represented by binomial distribution. The syntax is BINOM.DIST(number_s,trials,probability_s,cumulative) where number_s is the specified number of successes that we want, trials equals the number of trials we’ll look at, probability_s equals the probability of success in a trial. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Hypothesis Testing\n",
    "\n",
    "Hypothesis testing involves the careful construction of two statements: the null hypothesis and the alternative hypothesis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1. Null Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The null hypothesis reflects that there will be no observed effect in our experiment. This hypothesis is denoted by Ho. The null hypothesis is what we attempt to find evidence against in our hypothesis test. We hope to obtain a small enough p-value that it is lower than our level of significance alpha and we are justified in rejecting the null hypothesis. If our p-value is greater than alpha, then we fail to reject the null hypothesis. For example, if we are studying a new treatment, the null hypothesis is that our treatment will not change our subjects in any meaningful way. In other words, the treatment will not produce any effect in our subjects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2. Alternate Hypothesis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The alternative or experimental hypothesis reflects that there will be an observed effect for our experiment. This hypothesis is denoted by H1. The alternative hypothesis is what we are attempting to demonstrate in an indirect way by the use of our hypothesis test. If the null hypothesis is rejected, then we accept the alternative hypothesis. If the null hypothesis is not rejected, then we do not accept the alternative hypothesis. If we are studying a new treatment, then the alternative hypothesis is that our treatment does, in fact, change our subjects in a meaningful and measurable way."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3. Comparing sample mean with population mean (T - Test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is especially useful to compare a mean from a random sample to an established data source such as census data to ensure you have an unbiased sample. We will understand this with the help of an example. A random sample of 30 incoming college freshmen revealed the following statistics: mean age 19.5 years, standard deviation 1 year. The college database shows the mean age for previous incoming students was 18. Assumptions for comparing the sample and population mean: Random sampling, normal distribution. \n",
    "\n",
    "Let's first declare the hypotheses. \n",
    "Ho: There is no significant difference between the mean age of past college students and the mean age of current incoming college students. \n",
    "H1: There is a significant difference between the mean age of past college students and the mean age of current incoming college students.\n",
    "\n",
    "The second step is to set the rejection criteria. Significance level .05 alpha, 2-tailed test, Degrees of Freedom = n-1 or 29, Critical value from t-distribution = 2.045.\n",
    "\n",
    "Now, we will compute the test statistic and compare it with the critical value to decide whether to accept or reject the null hypothesis in favour of the alternate hypothesis.\n",
    "\n",
    "$$ Sx = S/\\sqrt{n} $$\n",
    "$$ Sx = 1/\\sqrt{30} $$\n",
    "$$ Sx = 0.183 $$\n",
    "\n",
    "Test statistic: \n",
    "$$ t = (sample mean - population mean) / Sx $$\n",
    "$$ t = (19.5 - 18)/ 0.183 $$\n",
    "$$ t = 8.197 $$\n",
    "\n",
    "Given that the test statistic (8.197) exceeds the critical value (2.045), the null hypothesis is rejected in favor of the alternative. There is a statistically significant difference between the mean age of the current class of incoming students and the mean age of freshman students from past years. If the results had not been significant, the null hypothesis would not have been rejected. This would be interpreted as the following: There is insufficient evidence to conclude there is a statistically significant difference in the ages of current and past freshman students.\n",
    "\n",
    "The following is an example in python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43.000112\n",
      "39.26\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_1sampResult(statistic=-2.5742714883655027, pvalue=0.013118685425061678)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "np.random.seed(6)\n",
    "population_ages1=stats.poisson.rvs(loc=18,mu=35,size=150000)\n",
    "population_ages2=stats.poisson.rvs(loc=18,mu=10,size=100000)\n",
    "population_ages=np.concatenate((population_ages1,population_ages2))\n",
    "state1_ages1=stats.poisson.rvs(loc=18,mu=30,size=30)\n",
    "state1_ages2=stats.poisson.rvs(loc=18,mu=10,size=20)\n",
    "state1_ages=np.concatenate((state1_ages1,state1_ages2))\n",
    "print(population_ages.mean())\n",
    "print(state1_ages.mean())\n",
    "stats.ttest_1samp(a=state1_ages,popmean=population_ages.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here as we can observe that the p-value is less than 0.05, there is sufficient evidence to reject the null hypothesis being that the population mean= sample mean. The conclusion is that the sample mean is diiferent from the population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4. Comparing two sample means (Two sample T - Test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will understand this concept with the help of an example. An experiment is conducted to determine whether intensive tutoring (covering a great deal of material in a fixed amount of time) is more effective than paced tutoring (covering less material in the same amount of time). Two randomly chosen groups are tutored separately and then administered proficiency tests. Use a significance level of α < 0.05, single tail.\n",
    "\n",
    "Very different means can occur by chance if there is great variation among the individual samples. In order to account for the variation, we take the difference of the sample means and divide by the standard error in order to standardize the difference. Since, we do not know the population standard deviation, we estimate it using the two sample standard deviations from our independent samples.\n",
    "\n",
    "Null Hypothesis: mean1 = mean2; Alternate hypothesis: mean1 - mean2 > 0\n",
    "\n",
    "Data: Sample 1: n= 12, mean1 = 46.31, s1 = 6.44. Sample 2: n= 10, mean2 = 42.79, s2 = 7.52.\n",
    "\n",
    "$$ t = (mean1 - mean2)/\\sqrt( s1^2/n1^2 + s2^2/n2^2) $$\n",
    "\n",
    "The number of degrees of freedom for the problem is smaller of n1-1 or n2-1.\n",
    "\n",
    "With the given data, t is calculated to be 1.66. The degrees of freedom parameter is the smaller of (12 – 1) and (10 – 1), or 9. Because this is a one‐tailed test, the alpha level (0.05) is not divided by two. In the t-table, the critical value is found to be 1.833. The computed t of 1.166 does not exceed the tabled value, therefore the null hypothesis cannot be rejected. This test has not provided statistically significant evidence that intensive tutoring is superior to paced tutoring. \n",
    "\n",
    "Let's see an example with python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42.8\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Ttest_indResult(statistic=-1.7083870793286842, pvalue=0.09073104343957748)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.seed(12)\n",
    "state2_ages1=stats.poisson.rvs(loc=18,mu=33,size=30)\n",
    "state2_ages2=stats.poisson.rvs(loc=18,mu=13,size=20)\n",
    "state2_ages=np.concatenate((state2_ages1,state2_ages2))\n",
    "print(state2_ages.mean())\n",
    "stats.ttest_ind(a=state1_ages,b=state2_ages,equal_var=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, p value is greater than 0.05 and therefore, the null hypothesis cannot be rejected. The conclusion is that there is no sufficient evidence to prove that the two sample means are different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.5. P - Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test statistics are helpful, but it can be more helpful to assign a p-value to these statistics. A p-value is the probability that, if the null hypothesis were true, we would observe a statistic at least as extreme as the one observed. In simple terms, p-value is the probability of your null hypothesis actually being observed. For example, p=0.05 implies that one in a twenty observations follow your null hypothesis. Typically, before we conduct a hypothesis test, we choose a threshold value. If we have any p-value that is less than or equal to this threshold, then we reject the null hypothesis. Otherwise we fail to reject the null hypothesis. This threshold is called the level of significance of our hypothesis test, and is denoted by the Greek letter alpha. There is no value of alpha that always defines statistical significance. However, the most common used criteria is that P < 0.05 is statistically significant and P < 0.001 is statistically highly significant.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.6. Test Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A test statistic is a statistic (a quantity derived from the sample) used in statistical hypothesis testing. A hypothesis test is typically specified in terms of a test statistic, considered as a numerical summary of a data-set that reduces the data to one value that can be used to perform the hypothesis test. In general, a test statistic is selected or defined in such a way as to quantify, within observed data, behaviours that would distinguish the null from the alternative hypothesis. There is a wide range of test statistics used and we will look into a few of the important ones."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z Value\n",
    "Simply put, a z-score is the number of standard deviations from the mean a data point is. Z-scores are a way to compare results from a test to a “normal” population. The basic z score formula for a sample is: z = (x – μ) / σ, where μ is the mean of the population and σ is its standard deviation. If you pick any point in a normal distribution and desire to know its location in relevance to the mean of the population, z-score comes in handy. Z-score gives the point's location in terms of how many standard deviations away from the mean."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CHI Square\n",
    "Chi Square is highly useful in comparing the observed results and expected results of an experiment. The chi-square value can be obtained with the formula $ chisquare=sum(i=1tok)((oi−ei)^2/ei) $\n",
    ", where o is the observed value, e is the expected value and i is i'th position in the contingency table. Lower the chi-square value, higher is the similarity between observed and expected phenomena. You could also take your calculated chi-square value and compare it to a critical value from a chi-square table. If the chi-square value is more than the critical value, then there is a significant difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F Value\n",
    "F values are generally used to compare the variances between two samples. The F value is given by between-group variability/ within group variability. Between group variability = $ sum(i=1 to k)Ni((Yi - Y)^2/k-1) $, where Ni is the number of observations in the i'th group, k is the number of groups, Yi is the sample mean of the i'th group and Y is the overall mean. Within group variability = $ sum(i=1 to k)sum(j=1 to Ni)((Yij - Yi)^2/Ni-k) $, where Yij is the jth sample in the i'th group. This F-statistic follows the F-distribution with degrees of freedom d1 = k-1 and d2 = Ni-k. The F-statistic compares the effects of all the variables together whereas the T-statistic only considers one variable.\n",
    "F-value should always be used along with the p-value. If F-value is greater than the F-critical value and p-value is less than p-critical value then the null hypothesis can be rejected in favour of the alternate hypothesis. When there is a conflict, p-value gets more importance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.7. Type I and Type II Errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A type I error is the rejection of a true null hypothesis (also known as a \"false positive\" finding or conclusion), while a type II error is the non-rejection of a false null hypothesis (also known as a \"false negative\" finding or conclusion). Much of statistical theory revolves around the minimization of one or both of these errors, though the complete elimination of either is treated as a statistical impossibility.\n",
    "\n",
    "The likelihood of type I error, is equal to the level of significance, that the researcher sets for his test. Here the level of significance refers to the chances of making type I error. Often, the significance level is set to 0.05 (5%), implying that it is acceptable to have a 5% probability of incorrectly rejecting the null hypothesis. The likelihood of making type II error is analogous to the power of the test. The power of the test is the probability that the test will find a statistically significant difference between men and women, as a function of the size of the true difference between those two populations. As the sample size increases, the power of test also increases, that results in the reduction in risk of making type II error. Power = 1 - risk of Type II error. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
