{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.a. Model representation:\n",
    "Linear Regression is a supervised machine learning algorithm where the predicted output is continuous and has a constant slope. It’s used to predict values within a continuous range, (e.g. sales, price) rather than trying to classify them into categories (e.g. 0 or 1). There are two main types:\n",
    "Simple linear regression uses traditional slope intercept form where there is a dependant variable y and an independent variable x. The dependent variable is related to the independent variable as y= mx+b\n",
    "Example: Suppose one wants to prdict weight of human being based on its height so height will be the independent variable and weight will be dependent variable.\n",
    "Weight can be explained as terms of height as\n",
    "Weight= Coeff*Height+Bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd  \n",
    "import numpy as np\n",
    "# initialize list of lists of height and weight\n",
    "data = [[48, 60],[52, 67],[70, 90],[61,79],[63,87],[65,81],[50,84],[51,63]]\n",
    "# create a dataframe\n",
    "df = pd.DataFrame(data, columns = ['Height_cm', 'Weight_Kg']) \n",
    "df \n",
    "df['Height_cm']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Building :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.07489451] 14.5685654008\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "lr = LinearRegression() #Build Linear Regression Model\n",
    "X=pd.DataFrame(df['Height_cm'])#Segregate independent Variable\n",
    "Y=pd.DataFrame(df['Weight_Kg'])# Segregate Dependent Variable\n",
    "lr.fit(X,Y)\n",
    "\n",
    "\n",
    "Coeff=lr.coef_[0]\n",
    "Bias=lr.intercept_[0]\n",
    "print(Coeff,Bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_weight(height, coeff, bias):\n",
    "    return coeff*height + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make Predictions :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    66.163502\n",
       "1    70.463080\n",
       "2    89.811181\n",
       "3    80.137131\n",
       "4    82.286920\n",
       "5    84.436709\n",
       "6    68.313291\n",
       "7    69.388186\n",
       "Name: Height_cm, dtype: float64"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_weight(df['Height_cm'],Coeff,Bias)#Predict Weight based on Linear regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.b. Cost Function :"
   ]
  },
  {
   "attachments": {
    "pic.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In linear regression model, our primary goal is to minimize the error in prediction values.We need to optimize our coeeficients of independent variable so that the error is reduced. This can be done by minimizing the error function through different iterations. For linear regression we normally use MSE- Mean squared error as the cost function.\n",
    "Given simple linear equation y=mx+b\n",
    "We can calculate MSE as\n",
    "\n",
    "![pic.png](attachment:pic.png)\n",
    "\n",
    "### Where ŷi=mxi+b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cost Function Code :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 46.52703059])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cost_function(height, weight, coeff, bias):\n",
    "    observations = len(height)\n",
    "    total_error = 0.0\n",
    "    for i1 in range(observations):\n",
    "        total_error += (weight[i1] - (coeff*height[i1] + bias))**2# Calculate error based on MSE formula\n",
    "    return total_error / observations\n",
    "cost_function(df['Height_cm'],df['Weight_Kg'],Coeff,Bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.c. Gradient Decent :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To minimize MSE we use Gradient Descent to calculate the gradient of our cost function. \n",
    "Math\n",
    "There are two parameters (coefficients) in our cost function we can control: Coefficient (m) of the independentg variable and \n",
    "and bias (b).Since we need to consider the impact each one has on the final prediction, we use partial derivatives. To find the partial derivatives, we use the Chain rule. We need the chain rule because (y−(mx+b))2\n",
    "is really 2 nested functions: the inner function y−(mx+b) and the outer function x2\n",
    "\n",
    "Partial derivatives can be calculated as \n",
    "df/dm=1/N ∑ -2xi(yi-(mxi+b))\n",
    "df/db=1/N ∑ -2(yi-(mxi+b)\n",
    "Given the learning rate of l we find the change in coefficient m as df/dm*l\n",
    "Given the learning rate of l we find the change in bias b as df/db*l\n",
    "\n",
    "We subtract these values from original values because the derivatives point in direction of steepest ascent\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.d. Gradient Decent for Linear Regression :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To solve for the gradient, we iterate through our data points using our new coefficient and bias values and take the average of the partial derivatives. The resulting gradient tells us the slope of our cost function at our current position (i.e. coefficient and bias) and the direction we should update to reduce our cost function (we move in the direction opposite the gradient). The size of our update is controlled by the learning rate.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Gradient Decent Code for Linear Regression :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 0.0445525 0.00076375 5557.70746557\n",
      "10 0.415381415204 0.00712505109337 2826.81011361\n",
      "20 0.678681663861 0.0116492014478 1450.04087508\n",
      "30 0.865633124393 0.0148689160789 755.94904347\n",
      "40 0.998374506486 0.0171624396647 406.025859783\n",
      "50 1.09262500121 0.0187983380092 229.613703434\n",
      "60 1.15954573635 0.0199673014122 140.676326642\n",
      "70 1.20706146918 0.0208047260781 95.8389541891\n",
      "80 1.2407990305 0.0214067476369 73.2343943553\n",
      "90 1.26475364991 0.0218416254569 61.8384038707\n",
      "100 1.28176206806 0.0221578258701 56.0931616298\n",
      "110 1.29383846144 0.0223897615204 53.1967179035\n",
      "120 1.30241295967 0.0225618664843 51.7364827334\n",
      "130 1.3085010005 0.0226914897365 51.0003054448\n",
      "140 1.31282357713 0.0227909496033 50.6291585785\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1.3156308186160144,\n",
       " 0.022861958830127246,\n",
       " [5557.7074655724891,\n",
       "  5193.1341380779122,\n",
       "  4852.6942259269154,\n",
       "  4534.7901856506869,\n",
       "  4237.930225291283,\n",
       "  3960.7213040399465,\n",
       "  3701.8625952736975,\n",
       "  3460.1393823148765,\n",
       "  3234.4173582689859,\n",
       "  3023.637303192349,\n",
       "  2826.8101136117043,\n",
       "  2643.0121610713654,\n",
       "  2471.3809579274994,\n",
       "  2311.1111100509197,\n",
       "  2161.4505374460832,\n",
       "  2021.6969450512313,\n",
       "  1891.1945271586044,\n",
       "  1769.3308899899316,\n",
       "  1655.5341779861244,\n",
       "  1549.270390326039,\n",
       "  1450.040875081839,\n",
       "  1357.379989252076,\n",
       "  1270.8529136919803,\n",
       "  1190.053612687348,\n",
       "  1114.6029285971404,\n",
       "  1044.1468026237469,\n",
       "  978.35461336172125,\n",
       "  916.91762532849384,\n",
       "  859.54754019664722,\n",
       "  805.97514392930043,\n",
       "  755.94904347015904,\n",
       "  709.23448706004604,\n",
       "  665.61226264414586,\n",
       "  624.87766920064144,\n",
       "  586.83955616362368,\n",
       "  551.31942643266814,\n",
       "  518.15059875989118,\n",
       "  487.17742558390324,\n",
       "  458.25456264028219,\n",
       "  431.24628692115198,\n",
       "  406.02585978333263,\n",
       "  382.47493221639115,\n",
       "  360.48298947976281,\n",
       "  339.94683250285703,\n",
       "  320.77009361457004,\n",
       "  302.86278432972819,\n",
       "  286.14087307040404,\n",
       "  270.52589084053358,\n",
       "  255.94456300342543,\n",
       "  242.32846543424415,\n",
       "  229.61370343394026,\n",
       "  217.74061189789546,\n",
       "  206.65347533230556,\n",
       "  196.30026640444675,\n",
       "  186.63240179995876,\n",
       "  177.60451424147536,\n",
       "  169.17423959878619,\n",
       "  161.30201809152442,\n",
       "  153.95090865150468,\n",
       "  147.08641557359408,\n",
       "  140.67632664165882,\n",
       "  134.69056196997741,\n",
       "  129.1010328507987,\n",
       "  123.88150994567208,\n",
       "  119.00750020202891,\n",
       "  114.45613191743287,\n",
       "  110.20604741215512,\n",
       "  106.23730280643167,\n",
       "  102.5312744320974,\n",
       "  99.070571439426899,\n",
       "  95.838954189080766,\n",
       "  92.821258046205713,\n",
       "  90.003322219085518,\n",
       "  87.371923308412136,\n",
       "  84.914713255349966,\n",
       "  82.620161397211987,\n",
       "  80.477500358836053,\n",
       "  78.476675525753805,\n",
       "  76.608297862050634,\n",
       "  74.86359985150925,\n",
       "  73.234394355288273,\n",
       "  71.713036193071602,\n",
       "  70.292386267403941,\n",
       "  68.965778062864146,\n",
       "  67.726986362869923,\n",
       "  66.570198037315436,\n",
       "  65.489984763960038,\n",
       "  64.481277555560297,\n",
       "  63.539342973213309,\n",
       "  62.659760914288555,\n",
       "  61.838403870717237,\n",
       "  61.07141756030714,\n",
       "  60.355202840192597,\n",
       "  59.686398817548387,\n",
       "  59.06186707831187,\n",
       "  58.478676959905862,\n",
       "  57.934091798853373,\n",
       "  57.425556088749488,\n",
       "  56.950683488328401,\n",
       "  56.507245623352823,\n",
       "  56.09316162977639,\n",
       "  55.706488389110852,\n",
       "  55.345411410176155,\n",
       "  55.008236314444744,\n",
       "  54.693380885025306,\n",
       "  54.39936764197418,\n",
       "  54.124816909094157,\n",
       "  53.868440339685407,\n",
       "  53.629034870868658,\n",
       "  53.405477078109023,\n",
       "  53.196717903450043,\n",
       "  53.001777732719511,\n",
       "  52.819741798606081,\n",
       "  52.649755888034896,\n",
       "  52.491022333699611,\n",
       "  52.342796270940347,\n",
       "  52.204382142402245,\n",
       "  52.075130434073003,\n",
       "  51.954434627382668,\n",
       "  51.841728353063225,\n",
       "  51.736482733412245,\n",
       "  51.638203900489209,\n",
       "  51.546430678597162,\n",
       "  51.460732420177138,\n",
       "  51.38070698495671,\n",
       "  51.305978852872968,\n",
       "  51.236197361911792,\n",
       "  51.171035062597049,\n",
       "  51.110186181404941,\n",
       "  51.053365185896418,\n",
       "  51.000305444831113,\n",
       "  50.950757976977684,\n",
       "  50.904490282747837,\n",
       "  50.861285253172305,\n",
       "  50.820940151098377,\n",
       "  50.783265659828047,\n",
       "  50.748084994733681,\n",
       "  50.715233073680729,\n",
       "  50.684555742365745,\n",
       "  50.655909050933943,\n",
       "  50.629158578482425,\n",
       "  50.604178802278248,\n",
       "  50.580852508732583,\n",
       "  50.559070243365724,\n",
       "  50.538729797182441,\n",
       "  50.519735727047788,\n",
       "  50.501998907811725,\n",
       "  50.485436114081899,\n",
       "  50.469969629681323,\n",
       "  50.455526882958374])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def update_coeff(height, weight, coeff, bias, learning_rate):\n",
    "    coeff_deriv = 0\n",
    "    bias_deriv = 0\n",
    "    observations = len(height)\n",
    "\n",
    "    for i in range(observations):\n",
    "        # Calculate partial derivatives\n",
    "        # -2x(y - (mx + b))\n",
    "        coeff_deriv += -2*height[i] * (weight[i] - (coeff*height[i] + bias))\n",
    "\n",
    "        # -2(y - (mx + b))\n",
    "        bias_deriv += -2*(weight[i] - (coeff*height[i] + bias))\n",
    "\n",
    "    # We subtract because the derivatives point in direction of steepest ascent\n",
    "    coeff -= (coeff_deriv / observations) * learning_rate\n",
    "    bias -= (bias_deriv / observations) * learning_rate\n",
    "\n",
    "    return coeff, bias\n",
    "def train(height, weight, coeff, bias, learning_rate, iters):\n",
    "    cost_history = []\n",
    "\n",
    "    for i in range(iters):\n",
    "        coeff,bias = update_coeff(height, weight, coeff, bias, learning_rate)\n",
    "\n",
    "        #Calculate cost for auditing purposes\n",
    "        cost = cost_function(height, weight, coeff, bias)\n",
    "        cost_history.append(cost)\n",
    "\n",
    "        # Log Progress\n",
    "        if i % 10 == 0:\n",
    "            print(i, coeff, bias, cost)\n",
    "\n",
    "    return coeff, bias, cost_history\n",
    "train(df['Height_cm'],df['Weight_Kg'],0,0,0.000005,150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, as the iterations increase the cost function value decreases . When i=140, it reduces to 50.62. So, this is how gradient descent helps us reducing the error iteration wise. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
