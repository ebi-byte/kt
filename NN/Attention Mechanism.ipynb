{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence.\n",
    "\n",
    "In a nutshell, attention in the deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\n",
    "\n",
    "The seq2seq model aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, question-answer dialog generation, or even parsing sentences into grammar trees. A critical and apparent disadvantage of it's fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born to resolve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to do seq2seq modeling without recurrent network units using the transformer models. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Value and Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of key-value pairs, , both of dimension (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query ( of dimension ) and the next output is produced by mapping this query and the set of keys and values.\n",
    "\n",
    "The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/multi-head-attention.png\" style=\"height: 350px;\" />  \n",
    "\n",
    "                                    Multi-head scaled dot-product attention mechanism\n",
    "                                   \n",
    "Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer-encoder.png\" style=\"height: 250px;\" />  \n",
    "\n",
    "                                       The transformer’s encoder\n",
    "\n",
    "The encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.  \n",
    "\n",
    "1. A stack of N=6 identical layers.\n",
    "2. Each layer has a multi-head self-attention layer and a simple position-wise fully connected feed-forward network.\n",
    "3. Each sub-layer adopts a residual connection and a layer normalization. All the sub-layers output data of the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer-decoder.png\" style=\"height: 300px;\" />  \n",
    "\n",
    "                                        The transformer's decoder\n",
    "\n",
    "The decoder is able to retrieval from the encoded representation.\n",
    "\n",
    "1. A stack of N = 6 identical layers\n",
    "2. Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.\n",
    "3. Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.\n",
    "4. The first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is the complete view of the transformer’s architecture:\n",
    "\n",
    "1. Both the source and target sequences first go through embedding layers to produce data of the same dimension .\n",
    "2. To preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.\n",
    "3. A softmax and linear layer are added to the final decoder output.\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" style=\"height: 500px;\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
