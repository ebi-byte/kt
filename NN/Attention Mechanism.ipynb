{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attention is, to some extent, motivated by how we pay visual attention to different regions of an image or correlate words in one sentence.\n",
    "\n",
    "In a nutshell, attention in the deep learning can be broadly interpreted as a vector of importance weights: in order to predict or infer one element, such as a pixel in an image or a word in a sentence, we estimate using the attention vector how strongly it is correlated with other elements and take the sum of their values weighted by the attention vector as the approximation of the target.\n",
    "\n",
    "The seq2seq model aims to transform an input sequence (source) to a new one (target) and both sequences can be of arbitrary lengths. Examples of transformation tasks include machine translation between multiple languages in either text or audio, question-answer dialog generation, or even parsing sentences into grammar trees. A critical and apparent disadvantage of it's fixed-length context vector design is incapability of remembering long sentences. Often it has forgotten the first part once it completes processing the whole input. The attention mechanism was born to resolve this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Applications\n",
    "\n",
    "1. **Attention in text translation**\n",
    "\n",
    "Given an input sequence of a sentence in French, translate and output a sentence in English. Attention is used to pay attention to specific words in the input sequence for each word in the output sequence.\n",
    "\n",
    "\n",
    "2. **Attention in Image Description**\n",
    "\n",
    "Different from the glimpse approach, the sequence-based attentional mechanism can be applied to computer vision problems to help get an idea of how to best use the convolutional neural network to pay attention to images when outputting a sequence, such as a caption. Given an input of an image, output an English description of the image. Attention is used to pay focus on different parts of the image for each word in the output sequence.\n",
    "\n",
    "![](https://insidebigdata.com/wp-content/uploads/2018/09/Eleks_6.png)\n",
    "\n",
    "#### Simple Working Example\n",
    "\n",
    "Gaussian attention works by exploiting parametrised one-dimensional Gaussian filters to create an image-sized attention map. As you can observe, attention vectors specify which part of the image should be attended to in x and y axis, respectively. The attention masks can be created as follows.\n",
    "\n",
    "<img src=\"https://akosiorek.github.io/ml/2017/10/14/hard_gauss.jpeg\" style=\"height: 300px;\" />  \n",
    "\n",
    "The code below lets you create matrix-valued masks for a mini-batch of samples in Tensorflow. If you want to create , you would call it as Ay = gaussian_mask(u, s, d, h, H), where u, s, d are and , in that order and specified in pixels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%` not found.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_v2_behavior()\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = 10, 10\n",
    "glimpse_size = 5, 5\n",
    "\n",
    "x = abs(np.random.randn(1, *img_size)) * .3\n",
    "x[0, 3:6, 3:6] = 1\n",
    "\n",
    "# Make a crop\n",
    "crop = x[0, 2:7, 2:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian_mask(u, s, d, R, C):\n",
    "    \"\"\"\n",
    "    :param u: tf.Tensor, centre of the first Gaussian.\n",
    "    :param s: tf.Tensor, standard deviation of Gaussians.\n",
    "    :param d: tf.Tensor, shift between Gaussian centres.\n",
    "    :param R: int, number of rows in the mask, there is one Gaussian per row.\n",
    "    :param C: int, number of columns in the mask.\n",
    "    \"\"\"\n",
    "    # indices to create centres\n",
    "    R = tf.to_float(tf.reshape(tf.range(R), (1, 1, R)))\n",
    "    C = tf.to_float(tf.reshape(tf.range(C), (1, C, 1)))\n",
    "    centres = u[np.newaxis, :, np.newaxis] + R * d\n",
    "    column_centres = C - centres\n",
    "    mask = tf.exp(-.5 * tf.square(column_centres / s))\n",
    "    # we add eps for numerical stability\n",
    "    normalised_mask = mask / (tf.reduce_sum(mask, 1, keep_dims=True) + 1e-8)\n",
    "    return normalised_mask\n",
    "\n",
    "def gaussian_glimpse(img_tensor, transform_params, crop_size):\n",
    "    \"\"\"\n",
    "    :param img_tensor: tf.Tensor of size (batch_size, Height, Width, channels)\n",
    "    :param transform_params: tf.Tensor of size (batch_size, 6), where params are  (mean_y, std_y, d_y, mean_x, std_x, d_x) specified in pixels.\n",
    "    :param crop_size): tuple of 2 ints, size of the resulting crop\n",
    "    \"\"\"\n",
    "    # parse arguments\n",
    "    h, w = crop_size\n",
    "    H, W = img_tensor.shape.as_list()[1:3]\n",
    "    split_ax = transform_params.shape.ndims -1\n",
    "    uy, sy, dy, ux, sx, dx = tf.split(transform_params, 6, split_ax)\n",
    "    # create Gaussian masks, one for each axis\n",
    "    Ay = gaussian_mask(uy, sy, dy, h, H)\n",
    "    Ax = gaussian_mask(ux, sx, dx, w, W)\n",
    "    # extract glimpse\n",
    "    glimpse = tf.matmul(tf.matmul(Ay, img_tensor, adjoint_a=True), Ax)\n",
    "    return glimpse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "# placeholders\n",
    "tx = tf.placeholder(tf.float32, x.shape)\n",
    "tu = tf.placeholder(tf.float32, [1])\n",
    "ts = tf.placeholder(tf.float32, [1])\n",
    "td = tf.placeholder(tf.float32, [1])\n",
    "stn_params = tf.placeholder(tf.float32, [1, 4], 'stn_params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Attention\n",
    "gaussian_att_params = tf.concat([tu, ts, td, tu, ts, td], -1)\n",
    "gaussian_glimpse_expr = gaussian_glimpse(tx, gaussian_att_params, glimpse_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "\n",
    "# extract a Gaussian glimpse\n",
    "u = 2\n",
    "s = .5\n",
    "d = 1\n",
    "u, s, d = (np.asarray([i]) for i in (u, s, d))\n",
    "gaussian_crop = sess.run(gaussian_glimpse_expr, feed_dict={tx: x, tu: u, ts: s, td: d})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAo0AAADECAYAAAD6ZRvAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAQ3UlEQVR4nO3df6zd5X0f8Pen2NjGgPmVuosxYamzZA5ro2ULJBsNFZUCoohIIEIIDdEyaVm6RYKFbmqbKZB1aEmVJVWpWNJlrXBLaBFtiaptRGrSiUK3dC0LSQgb4YdsjLEx2GC4NvX1sz/O90o3ru3Hvt84x1y/XpLl4/N9Puf5HNvf+7zPc77n3mqtBQAADuVHpt0AAADHPqERAIAuoREAgC6hEQCALqERAIAuoREAgC6hEQB4zauq/1pV10+7j8XsuAiNVfVkVf3MD2GeT1bVhmOhF+DAquraqvqLqtpVVc8MC80/nnZf8FpTVddU1f+sqperautw+6NVVdPop7V2aWvtt4/W41fVRVXVquoX9rv/3OH+JfPu+1BV3X+0epmW4yI0AiRJVd2Y5HNJ/n2S1UnOSfIbSa44wNgl+98HTFTVv0ry+SSfSfJjmZxPH0nyj5KcOMXWjqbrkzw//H5cOu5C41z6r6pfraoXquqJqrp03vGvV9WtVfW/qmpnVf1RVZ0xHLuoqjbt93hPVtXPVNUlSX4xyfuGHYz/c5i9/FlV/ceq2lFVj1fVu4b7Nw6v3K6fN/6yqvqrqnpxOP7J/R7vg1X1VFVtr6pPzN/VrKofqap/U1XfG47/3tzzguNBVa1KckuSn2+t3dNae7m19tetta+01m4a3im4u6o2VNWLST5UVcuq6nNVtXn49bmqWjY83kVVtamqfrGqnhvOtw9M9UnCD8G8c+mjrbW7W2svtYm/aq19oLW2Zxh30DXrUOvpcPsdwzsCL1bVs1X12eH+5cM5un1YN79RVauHY1+vqn863P7xqvqTYdxzVfU7VXXafnN9vKq+Oaz1d1XV8kM855OSXJXk55O8qar+wbzD/2P4fcew/r8zye1J3jn8ecfC/qaPPcddaBycn+TRJGcl+XSS/7zfdvoHk/yTJK9PsjfJr/UesLX23zLZvbirtXZya+0nj6CXbyY5M8nvJvlykn+YZF2S65L8elWdPIx9eejttCSXJfnnVfXeJKmq9ZnsmHwgyd9KsirJmnnzfCzJe5O8e3heLyS57TB7hMXgnUmWJ/mDQ4y5IsndmZxjv5Pkl5JckORtSX4yyTuS/PK88T+WydeRNZnsPnyhqt78A+8cji3vTLIsyR91xh10zToMn0/y+dbaqUl+PMnvDfdfn8n6tjaTdfMjSWYOUF9Jbs1kvfu7w/hP7jfm6iSXJPnbSX4iyYcO0c+VSXYl+f0k/314XnN+avj9tGH9f3Do68Hhz6dlkTheQ+NTrbUvttZmk/x2JiFr9bzjd7TWvtVaeznJJ5JcXVUnHKVenmit/Zehl7sy+Y99S2ttT2vtviSvZhIg01r7emvt4dbavtbaN5PcmUkITCavgL7SWru/tfZqkn+bZP4PFv9nSX6ptbZpeBX4ySRXeQuO48iZSZ5rre09xJgHW2t/OJxjM5m8CLultba1tbYtyc1Jfm6/mk8M5+ufJvnjTBYiWMzOyn7nUlU9MOz8zVTVTyXdNavnr5Osq6qzWmu7Wmt/Pu/+M5Osa63Nttb+d2vtxf2LW2uPtda+Opyb25J89gBz/1prbXNr7fkkX8nkxeHBXJ/JptBsJhs876+qpYf5XBaN4zU0bpm70Vp7Zbh58rzjG+fdfirJ0kxOkqPh2Xm3Z4ae9r/v5CSpqvOr6mtVta2qdmbySmaur9fP73t4XtvnPc4bkvzBcFLvSPJIktl8f1iGxWx7krM6L5Q27vfn12fyNWDOU8N9c14YXlwe7DgsRn/jXGqtvWvYUdueIVt01qyeDyf5O0m+O7wF/bPD/XdkstP35eGSkU8fKLxV1Y9W1Zer6unhcpMNB5h7y7zbr+T7c8D8x1qb5KczefchmeywLs9k9/S4cryGxp61826fk8krm+cy2Wo/ae7AsPv4unlj5+/sHQ2/m+TeJGtba6syuWZi7m31Z5KcPa+3FZm8GpuzMcmlrbXT5v1a3lp7+ij3DMeKB5PszuQyjYPZ/xzenMkLrjnnDPfNOb2qVh7iOCxGDybZkwN8gGw/h1qzDrmettb+X2vt/Ul+NMl/SHJ3Va0crkO+ubW2Psm7kvxsvv+t4jm3ZnI+/8TwFvd18+Y+Uj+XSV76SlVtSfJ4JqFxbt4Drf1HOw9MhdB4YNdV1frhwtdbktw9bEn/3yTLh4t7l2ZybdOyeXXPJjm3qo7W3+spSZ5vre2uqnckuXbesbuTXF6TD9KcmMnbaPNPkNuT/EpVvSFJqup1VdU74WHRaK3tzOSyjduq6r1VdVJVLa2qS6vq0wcpuzPJLw/ny1lD/f7fVuvmqjqxqi7MZAH7/aP2JOAY0Frbkcka8xtVdVVVnVyTD1u+Lcn8F1GHWrMOuZ5W1XVV9brW2r4kcx8kma2qn66qvzeEzBcz2dSZPUCbp2RyDeKOqlqT5KYRT/mDw/N927xfVya5rKrOTLItyb4kb5xX82ySs4f1eNEQGg/sjiS/lcnW9fJMPkQyt+h8NMlvJnk6k1dK8z/9NbdYbK+qvzwKfX00yS1V9VImi9fchcFprX07yb/M5IM0zyR5KcnWTF4NJpOLiu9Nct9Q/+eZfAgHjhuttc8muTGTBWpbJjvw/yLJHx6k5N8l+YtMPqz2cJK/HO6bsyWTD5VtzuStq4+01r57VJqHY0hr7dOZnEu/kMla82yS/5TkXyd5YBh2qDWrt55ekuTbVbUrk/Xrmtba7kw+fHZ3JoHxkSR/mr/5Qi6ZhLy/n2RnJtca37OQ51lVFyQ5N8ltrbUt837dm+SxJO8fLgf7lSR/NlwCdkGSP0ny7SRbquq5hcx9LKrWFuUO6oJV1deTbGit/ea0exlj+MT1jiRvaq09Me1+YLGpqosy+Vpxdm8swGJgp3ERqarLh7fcVib51Ux2Rp6cblcAwGIgNC4uV2TyNtnmJG/KZDvfVjIAMJq3pwEA6LLTCABA1xH9NJAlS5a0ZcuW9QcexCuvvNIf1DFm/iRZtWrV6B62b9/eH3QIp59++qj6l19+uT+oY9++faPqx/47JMnu3bsXXLt3797Mzs4u9HtuHRdWrFjRTj311Kn2sHXr1qnOf/bZ0/+MyjPPPDPtFvL9PyX1h292djb79u1zvh7C8uXL28qVK/sDj6I9e/b0Bx1F0/56lSS7du2adgtT/3c41Pp6RKFx2bJlWb9+/YIbeeihhxZcO2ft2rX9QYdw6aWXju7hjjvuGFV/xRXjvj3iN77xjVH1ybjAliRvfOMb+4M6vvvdhX9nks2bff/knlNPPTXXXHPNVHu47bbp/njzG2+8carzJ8nNN9887RaydOl0f9rZjh07+oOOcytXrsxll033B4x873vfm+r8F1988VTnT5IHHnigP+goe+KJ6X7Dk02bNh30mLenAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoWnIkg0844YScfvrpC57syiuvXHDtnHvuuWdU/Z49e0b38Ja3vGVU/U033TSq/j3vec+o+h+EW2+9dfRjfOlLX1pw7V133TV6fgDg8NlpBACgS2gEAKBLaAQAoEtoBACgS2gEAKBLaAQAoEtoBACgS2gEAKBLaAQAoEtoBACgS2gEAKBLaAQAoEtoBACgS2gEAKBLaAQAoEtoBACga8mRDF6xYkXOO++8BU82pnbO6tWrR9Xfdttto3v4zGc+M6r+/PPPH1W/dOnSUfVJcuGFF46qv+CCC0b38O53v3vBtbt37x49/2I3MzOT73znO1PtYe/evVOd/1hwww03TLuFH8jX3jF27do11flfC1pr2bNnz1R7+OIXvzjV+desWTPV+ZPk6quvnnYL+djHPjbV+bdt23bQY3YaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6FpyJIP37t2brVu3LniyT33qUwuunbNu3bpR9Xv37h3dw1g33HDDqPrzzjtvdA9vfetbR9Xfd999o3vYt2/fgmtba6PnBwAOn51GAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6lhzJ4N27d+fRRx9d8GRPP/30gmvnnHvuuaMf47XuySefHP0YmzZtGlW/b9++0T201kY/BgfXWsvevXun3QbHgB/E14wxXn311anO/1ox7a+Jq1evnur8q1atmur8ySTnTNvs7OxU5z/U/0M7jQAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHQtOdKCffv2LXiy973vfQuunbNu3brRj/Fat2HDhtGPce21146qv/jii0f38NWvfnXBta+++uro+QGAw2enEQCALqERAIAuoREAgC6hEQCALqERAIAuoREAgC6hEQCALqERAIAuoREAgC6hEQCALqERAIAuoREAgC6hEQCALqERAIAuoREAgK4lRzL45JNPzoUXXrjgyb7whS8suHbOww8/PKr+jDPOGN3DOeecM/oxxti5c+fox1izZs2o+o0bN47u4frrr19w7T333DN6/sXulFNOyUUXXTTtNjgGbNiwYarzf/zjH5/q/K8FJ510Ut7+9rdPtYcdO3ZMdf7Z2dmpzp8kMzMz024hl19++VTnf/zxxw96zE4jAABdQiMAAF1CIwAAXUIjAABdQiMAAF1CIwAAXUIjAABdQiMAAF1CIwAAXUIjAABdQiMAAF1CIwAAXUIjAABdQiMAAF1CIwAAXUIjAABdS45k8K5du3L//fcveLLTTjttwbVz7rzzzlH1J5xwwuge3vzmN4+q37Rp06j6K664YlR9kjzyyCOj6u+9997RPaxYsWLBtTMzM6PnBwAOn51GAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAuoRGAAC6hEYAALqERgAAupYcyeCqytKlSxc82YoVKxZcO2fjxo2j6q+66qrRPdx+++2j6q+55ppR9Q888MCo+iR5/vnnR9Vfcsklo3v42te+tuDamZmZ0fMvdmeeeWauu+66qfZwxhlnTHX+c845Z6rzHyt27tw51fk3b9481flfC0488cSsXbt2qj18+MMfnur8y5Ytm+r8SdJam3YLeemll6Y6/wsvvHDQY3YaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoEhoBAOgSGgEA6BIaAQDoWnIkg2dmZvLwww8frV4Oy7e+9a1R9atWrRrdwymnnDKqfufOnaPqt2zZMqo+SR599NFR9Q899NDoHtavX7/g2scee2z0/ADA4bPTCABAl9AIAECX0AgAQJfQCABAl9AIAECX0AgAQJfQCABAl9AIAECX0AgAQJfQCABAl9AIAECX0AgAQJfQCABAl9AIAECX0AgAQFe11g5/cNW2JE8dvXbgsL2htfa6aTdxLHO+cgxxvnY4XzmGHPR8PaLQCADA8cnb0wAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHQJjQAAdAmNAAB0CY0AAHT9f4P9MOQ/dchCAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x216 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(12, 3))\n",
    "\n",
    "titles = ['Input Image', 'Crop', 'Gaussian Att']\n",
    "imgs = [x, crop, gaussian_crop]\n",
    "for ax, title, img in zip(axes, titles, imgs):\n",
    "    ax.imshow(img.squeeze(), cmap='gray', vmin=0., vmax=1.)\n",
    "    ax.set_title(title)\n",
    "    ax.xaxis.set_visible(False)\n",
    "    ax.yaxis.set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. **Attention in Entailment**\n",
    "\n",
    "Given a premise scenario and a hypothesis about the scenario in English, output whether the premise contradicts, is not related, or entails the hypothesis.  \n",
    "For example:  \n",
    "\n",
    "premise: “A wedding party taking pictures“  \n",
    "hypothesis: “Someone got married“  \n",
    "\n",
    "Attention is used to relate each word in the hypothesis to words in the premise, and vise-versa."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Self Attention Mechanism\n",
    "Self-attention, also known as intra-attention, is an attention mechanism relating different positions of a single sequence in order to compute a representation of the same sequence. It has been shown to be very useful in machine reading, abstractive summarization, or image description generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is actually possible to do seq2seq modeling without recurrent network units using the transformer models. The proposed “transformer” model is entirely built on the self-attention mechanisms without using sequence-aligned recurrent architecture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Value and Query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The major component in the transformer is the unit of multi-head self-attention mechanism. The transformer views the encoded representation of the input as a set of key-value pairs, , both of dimension (input sequence length); in the context of NMT, both the keys and values are the encoder hidden states. In the decoder, the previous output is compressed into a query ( of dimension ) and the next output is produced by mapping this query and the set of keys and values.\n",
    "\n",
    "*The attention weights are the relevance scores of the input encoder hidden states (values), in processing the decoder state (query). This is calculated using the encoder hidden states (keys) and the decoder hidden state (query).*\n",
    "\n",
    "The transformer adopts the scaled dot-product attention: the output is a weighted sum of the values, where the weight assigned to each value is determined by the dot-product of the query with all the keys."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-Head Self-Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/multi-head-attention.png\" style=\"height: 350px;\" />  \n",
    "\n",
    "                                    Multi-head scaled dot-product attention mechanism\n",
    "                                   \n",
    "Rather than only computing the attention once, the multi-head mechanism runs through the scaled dot-product attention multiple times in parallel. The independent attention outputs are simply concatenated and linearly transformed into the expected dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer-encoder.png\" style=\"height: 250px;\" />  \n",
    "\n",
    "                                       The transformer’s encoder\n",
    "\n",
    "The encoder generates an attention-based representation with capability to locate a specific piece of information from a potentially infinitely-large context.  \n",
    "\n",
    "1. A stack of N=6 identical layers.\n",
    "2. Each layer has a multi-head self-attention layer and a simple position-wise fully connected feed-forward network.\n",
    "3. Each sub-layer adopts a residual connection and a layer normalization. All the sub-layers output data of the same dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer-decoder.png\" style=\"height: 300px;\" />  \n",
    "\n",
    "                                        The transformer's decoder\n",
    "\n",
    "The decoder is able to retrieval from the encoded representation.\n",
    "\n",
    "1. A stack of N = 6 identical layers\n",
    "2. Each layer has two sub-layers of multi-head attention mechanisms and one sub-layer of fully-connected feed-forward network.\n",
    "3. Similar to the encoder, each sub-layer adopts a residual connection and a layer normalization.\n",
    "4. The first multi-head attention sub-layer is modified to prevent positions from attending to subsequent positions, as we don’t want to look into the future of the target sequence when predicting the current position."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally here is the complete view of the transformer’s architecture:\n",
    "\n",
    "1. Both the source and target sequences first go through embedding layers to produce data of the same dimension .\n",
    "2. To preserve the position information, a sinusoid-wave-based positional encoding is applied and summed with the embedding output.\n",
    "3. A softmax and linear layer are added to the final decoder output.\n",
    "\n",
    "<img src=\"https://lilianweng.github.io/lil-log/assets/images/transformer.png\" style=\"height: 500px;\" />  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following is an example where the transformer model is used for Machine Translation. The data is derived from http://www.manythings.org/anki/fra-eng.zip. The model translates English into French. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "import requests\n",
    "from zipfile import ZipFile\n",
    "import time\n",
    "\n",
    "\n",
    "# Mode can be either 'train' or 'infer'\n",
    "# Set to 'infer' will skip the training\n",
    "MODE = 'train'\n",
    "URL = 'http://www.manythings.org/anki/fra-eng.zip'\n",
    "FILENAME = 'fra-eng.zip'\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "\n",
    "def maybe_download_and_read_file(url, filename):\n",
    "    \"\"\" Download and unzip training data\n",
    "\n",
    "    Args:\n",
    "        url: data url\n",
    "        filename: zip filename\n",
    "    \n",
    "    Returns:\n",
    "        Training data: an array containing text lines from the data\n",
    "    \"\"\"\n",
    "    if not os.path.exists(filename):\n",
    "        session = requests.Session()\n",
    "        response = session.get(url, stream=True)\n",
    "\n",
    "        CHUNK_SIZE = 32768\n",
    "        with open(filename, \"wb\") as f:\n",
    "            for chunk in response.iter_content(CHUNK_SIZE):\n",
    "                if chunk:\n",
    "                    f.write(chunk)\n",
    "\n",
    "    zipf = ZipFile(filename)\n",
    "    filename = zipf.namelist()\n",
    "    with zipf.open('fra.txt') as f:\n",
    "        lines = f.read()\n",
    "\n",
    "    return lines\n",
    "\n",
    "\n",
    "#Load the data\n",
    "lines = open('fra.txt', encoding='UTF-8').read().strip()\n",
    "raw_data = []\n",
    "for line in lines.split('\\n'):\n",
    "    raw_data.append(line.split('\\t'))\n",
    "\n",
    "print(raw_data[-5:])\n",
    "# The last element is empty, so omit it\n",
    "raw_data = raw_data[:-1]\n",
    "\n",
    "\n",
    "\"\"\"## Preprocessing\"\"\"\n",
    "\n",
    "\n",
    "def unicode_to_ascii(s):\n",
    "    return ''.join(\n",
    "        c for c in unicodedata.normalize('NFD', s)\n",
    "        if unicodedata.category(c) != 'Mn')\n",
    "\n",
    "\n",
    "def normalize_string(s):\n",
    "    s = unicode_to_ascii(s)\n",
    "    s = re.sub(r'([!.?])', r' \\1', s)\n",
    "    s = re.sub(r'[^a-zA-Z.!?]+', r' ', s)\n",
    "    s = re.sub(r'\\s+', r' ', s)\n",
    "    return s\n",
    "\n",
    "\n",
    "raw_data_en, raw_data_fr, info = list(zip(*raw_data))\n",
    "raw_data_en = [normalize_string(data) for data in raw_data_en]\n",
    "raw_data_fr_in = ['<start> ' + normalize_string(data) for data in raw_data_fr]\n",
    "raw_data_fr_out = [normalize_string(data) + ' <end>' for data in raw_data_fr]\n",
    "\n",
    "\"\"\"## Tokenization\"\"\"\n",
    "\n",
    "en_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "en_tokenizer.fit_on_texts(raw_data_en)\n",
    "data_en = en_tokenizer.texts_to_sequences(raw_data_en)\n",
    "data_en = tf.keras.preprocessing.sequence.pad_sequences(data_en,\n",
    "                                                        padding='post')\n",
    "\n",
    "fr_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "fr_tokenizer.fit_on_texts(raw_data_fr_in)\n",
    "fr_tokenizer.fit_on_texts(raw_data_fr_out)\n",
    "data_fr_in = fr_tokenizer.texts_to_sequences(raw_data_fr_in)\n",
    "data_fr_in = tf.keras.preprocessing.sequence.pad_sequences(data_fr_in,\n",
    "                                                           padding='post')\n",
    "\n",
    "data_fr_out = fr_tokenizer.texts_to_sequences(raw_data_fr_out)\n",
    "data_fr_out = tf.keras.preprocessing.sequence.pad_sequences(data_fr_out,\n",
    "                                                            padding='post')\n",
    "\n",
    "\"\"\"## Create tf.data.Dataset object\"\"\"\n",
    "\n",
    "BATCH_SIZE = 64\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "    (data_en, data_fr_in, data_fr_out))\n",
    "dataset = dataset.shuffle(len(data_en)).batch(BATCH_SIZE)\n",
    "\n",
    "\"\"\"## Create the Positional Embedding\"\"\"\n",
    "\n",
    "\n",
    "def positional_encoding(pos, model_size):\n",
    "    \"\"\" Compute positional encoding for a particular position\n",
    "\n",
    "    Args:\n",
    "        pos: position of a token in the sequence\n",
    "        model_size: depth size of the model\n",
    "    \n",
    "    Returns:\n",
    "        The positional encoding for the given token\n",
    "    \"\"\"\n",
    "    PE = np.zeros((1, model_size))\n",
    "    for i in range(model_size):\n",
    "        if i % 2 == 0:\n",
    "            PE[:, i] = np.sin(pos / 10000 ** (i / model_size))\n",
    "        else:\n",
    "            PE[:, i] = np.cos(pos / 10000 ** ((i - 1) / model_size))\n",
    "    return PE\n",
    "\n",
    "max_length = max(len(data_en[0]), len(data_fr_in[0]))\n",
    "MODEL_SIZE = 128\n",
    "\n",
    "pes = []\n",
    "for i in range(max_length):\n",
    "    pes.append(positional_encoding(i, MODEL_SIZE))\n",
    "\n",
    "pes = np.concatenate(pes, axis=0)\n",
    "pes = tf.constant(pes, dtype=tf.float32)\n",
    "\n",
    "\n",
    "print(pes.shape)\n",
    "print(data_en.shape)\n",
    "print(data_fr_in.shape)\n",
    "\n",
    "\n",
    "\"\"\"## Create the Multihead Attention layer\"\"\"\n",
    "\n",
    "\n",
    "class MultiHeadAttention(tf.keras.Model):\n",
    "    \"\"\" Class for Multi-Head Attention layer\n",
    "\n",
    "    Attributes:\n",
    "        key_size: d_key in the paper\n",
    "        h: number of attention heads\n",
    "        wq: the Linear layer for Q\n",
    "        wk: the Linear layer for K\n",
    "        wv: the Linear layer for V\n",
    "        wo: the Linear layer for the output\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, h):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.key_size = model_size // h\n",
    "        self.h = h\n",
    "        self.wq = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wk = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(key_size) for _ in range(h)]\n",
    "        self.wv = tf.keras.layers.Dense(model_size) #[tf.keras.layers.Dense(value_size) for _ in range(h)]\n",
    "        self.wo = tf.keras.layers.Dense(model_size)\n",
    "\n",
    "    def call(self, query, value, mask=None):\n",
    "        \"\"\" The forward pass for Multi-Head Attention layer\n",
    "\n",
    "        Args:\n",
    "            query: the Q matrix\n",
    "            value: the V matrix, acts as V and K\n",
    "            mask: mask to filter out unwanted tokens\n",
    "                  - zero mask: mask for padded tokens\n",
    "                  - right-side mask: mask to prevent attention towards tokens on the right-hand side\n",
    "        \n",
    "        Returns:\n",
    "            The concatenated context vector\n",
    "            The alignment (attention) vectors of all heads\n",
    "        \"\"\"\n",
    "        # query has shape (batch, query_len, model_size)\n",
    "        # value has shape (batch, value_len, model_size)\n",
    "        query = self.wq(query)\n",
    "        key = self.wk(value)\n",
    "        value = self.wv(value)\n",
    "        \n",
    "        # Split matrices for multi-heads attention\n",
    "        batch_size = query.shape[0]\n",
    "        \n",
    "        # Originally, query has shape (batch, query_len, model_size)\n",
    "        # We need to reshape to (batch, query_len, h, key_size)\n",
    "        query = tf.reshape(query, [batch_size, -1, self.h, self.key_size])\n",
    "        # In order to compute matmul, the dimensions must be transposed to (batch, h, query_len, key_size)\n",
    "        query = tf.transpose(query, [0, 2, 1, 3])\n",
    "        \n",
    "        # Do the same for key and value\n",
    "        key = tf.reshape(key, [batch_size, -1, self.h, self.key_size])\n",
    "        key = tf.transpose(key, [0, 2, 1, 3])\n",
    "        value = tf.reshape(value, [batch_size, -1, self.h, self.key_size])\n",
    "        value = tf.transpose(value, [0, 2, 1, 3])\n",
    "        \n",
    "        # Compute the dot score\n",
    "        # and divide the score by square root of key_size\n",
    "        # (must convert key_size to float32 otherwise an error would occur)\n",
    "        score = tf.matmul(query, key, transpose_b=True) / tf.math.sqrt(tf.dtypes.cast(self.key_size, dtype=tf.float32))\n",
    "        # score will have shape of (batch, h, query_len, value_len)\n",
    "        \n",
    "        # Mask out the score if a mask is provided\n",
    "        # There are two types of mask:\n",
    "        # - Padding mask (batch, 1, 1, value_len): to prevent attention being drawn to padded token (i.e. 0)\n",
    "        # - Look-left mask (batch, 1, query_len, value_len): to prevent decoder to draw attention to tokens to the right\n",
    "        if mask is not None:\n",
    "            score *= mask\n",
    "\n",
    "            # We want the masked out values to be zeros when applying softmax\n",
    "            # One way to accomplish that is assign them to a very large negative value\n",
    "            score = tf.where(tf.equal(score, 0), tf.ones_like(score) * -1e9, score)\n",
    "        \n",
    "        # Alignment vector: (batch, h, query_len, value_len)\n",
    "        alignment = tf.nn.softmax(score, axis=-1)\n",
    "        \n",
    "        # Context vector: (batch, h, query_len, key_size)\n",
    "        context = tf.matmul(alignment, value)\n",
    "        \n",
    "        # Finally, do the opposite to have a tensor of shape (batch, query_len, model_size)\n",
    "        context = tf.transpose(context, [0, 2, 1, 3])\n",
    "        context = tf.reshape(context, [batch_size, -1, self.key_size * self.h])\n",
    "        \n",
    "        # Apply one last full connected layer (WO)\n",
    "        heads = self.wo(context)\n",
    "        \n",
    "        return heads, alignment\n",
    "\n",
    "\n",
    "\"\"\"## Create the Encoder\"\"\"\n",
    "\n",
    "\n",
    "class Encoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Encoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention: array of Multi-Head Attention layers\n",
    "        attention_dropout: array of Dropout layers for Multi-Head Attention\n",
    "        attention_norm: array of LayerNorm layers for Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "\n",
    "        self.attention_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "    def call(self, sequence, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Encoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        sub_in = embed_out\n",
    "        alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            sub_out, alignment = self.attention[i](sub_in, sub_in, encoder_mask)\n",
    "            sub_out = self.attention_dropout[i](sub_out, training=training)\n",
    "            sub_out = sub_in + sub_out\n",
    "            sub_out = self.attention_norm[i](sub_out)\n",
    "            \n",
    "            alignments.append(alignment)\n",
    "            ffn_in = sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_in + ffn_out\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            sub_in = ffn_out\n",
    "\n",
    "        return ffn_out, alignments\n",
    "\n",
    "\n",
    "H = 8\n",
    "NUM_LAYERS = 4\n",
    "vocab_size = len(en_tokenizer.word_index) + 1\n",
    "encoder = Encoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "print(vocab_size)\n",
    "sequence_in = tf.constant([[1, 2, 3, 0, 0]])\n",
    "encoder_output, _ = encoder(sequence_in)\n",
    "encoder_output.shape\n",
    "\n",
    "\n",
    "class Decoder(tf.keras.Model):\n",
    "    \"\"\" Class for the Decoder\n",
    "\n",
    "    Args:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        num_layers: number of layers (Multi-Head Attention + FNN)\n",
    "        h: number of attention heads\n",
    "        embedding: Embedding layer\n",
    "        embedding_dropout: Dropout layer for Embedding\n",
    "        attention_bot: array of bottom Multi-Head Attention layers (self attention)\n",
    "        attention_bot_dropout: array of Dropout layers for bottom Multi-Head Attention\n",
    "        attention_bot_norm: array of LayerNorm layers for bottom Multi-Head Attention\n",
    "        attention_mid: array of middle Multi-Head Attention layers\n",
    "        attention_mid_dropout: array of Dropout layers for middle Multi-Head Attention\n",
    "        attention_mid_norm: array of LayerNorm layers for middle Multi-Head Attention\n",
    "        dense_1: array of first Dense layers for FFN\n",
    "        dense_2: array of second Dense layers for FFN\n",
    "        ffn_dropout: array of Dropout layers for FFN\n",
    "        ffn_norm: array of LayerNorm layers for FFN\n",
    "\n",
    "        dense: Dense layer to compute final output\n",
    "    \"\"\"\n",
    "    def __init__(self, vocab_size, model_size, num_layers, h):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.model_size = model_size\n",
    "        self.num_layers = num_layers\n",
    "        self.h = h\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, model_size)\n",
    "        self.embedding_dropout = tf.keras.layers.Dropout(0.1)\n",
    "        self.attention_bot = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_bot_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_bot_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "        self.attention_mid = [MultiHeadAttention(model_size, h) for _ in range(num_layers)]\n",
    "        self.attention_mid_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.attention_mid_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense_1 = [tf.keras.layers.Dense(\n",
    "            MODEL_SIZE * 4, activation='relu') for _ in range(num_layers)]\n",
    "        self.dense_2 = [tf.keras.layers.Dense(\n",
    "            model_size) for _ in range(num_layers)]\n",
    "        self.ffn_dropout = [tf.keras.layers.Dropout(0.1) for _ in range(num_layers)]\n",
    "        self.ffn_norm = [tf.keras.layers.LayerNormalization(\n",
    "            epsilon=1e-6) for _ in range(num_layers)]\n",
    "\n",
    "        self.dense = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "    def call(self, sequence, encoder_output, training=True, encoder_mask=None):\n",
    "        \"\"\" Forward pass for the Decoder\n",
    "\n",
    "        Args:\n",
    "            sequence: source input sequences\n",
    "            encoder_output: output of the Encoder (for computing middle attention)\n",
    "            training: whether training or not (for Dropout)\n",
    "            encoder_mask: padding mask for the Encoder's Multi-Head Attention\n",
    "        \n",
    "        Returns:\n",
    "            The output of the Encoder (batch_size, length, model_size)\n",
    "            The bottom alignment (attention) vectors for all layers\n",
    "            The middle alignment (attention) vectors for all layers\n",
    "        \"\"\"\n",
    "        # EMBEDDING AND POSITIONAL EMBEDDING\n",
    "        embed_out = self.embedding(sequence)\n",
    "\n",
    "        embed_out *= tf.math.sqrt(tf.cast(self.model_size, tf.float32))\n",
    "        embed_out += pes[:sequence.shape[1], :]\n",
    "        embed_out = self.embedding_dropout(embed_out)\n",
    "\n",
    "        bot_sub_in = embed_out\n",
    "        bot_alignments = []\n",
    "        mid_alignments = []\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            # BOTTOM MULTIHEAD SUB LAYER\n",
    "            seq_len = bot_sub_in.shape[1]\n",
    "\n",
    "            if training:\n",
    "                mask = tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "            else:\n",
    "                mask = None\n",
    "            bot_sub_out, bot_alignment = self.attention_bot[i](bot_sub_in, bot_sub_in, mask)\n",
    "            bot_sub_out = self.attention_bot_dropout[i](bot_sub_out, training=training)\n",
    "            bot_sub_out = bot_sub_in + bot_sub_out\n",
    "            bot_sub_out = self.attention_bot_norm[i](bot_sub_out)\n",
    "            \n",
    "            bot_alignments.append(bot_alignment)\n",
    "\n",
    "            # MIDDLE MULTIHEAD SUB LAYER\n",
    "            mid_sub_in = bot_sub_out\n",
    "\n",
    "            mid_sub_out, mid_alignment = self.attention_mid[i](\n",
    "                mid_sub_in, encoder_output, encoder_mask)\n",
    "            mid_sub_out = self.attention_mid_dropout[i](mid_sub_out, training=training)\n",
    "            mid_sub_out = mid_sub_out + mid_sub_in\n",
    "            mid_sub_out = self.attention_mid_norm[i](mid_sub_out)\n",
    "            \n",
    "            mid_alignments.append(mid_alignment)\n",
    "\n",
    "            # FFN\n",
    "            ffn_in = mid_sub_out\n",
    "\n",
    "            ffn_out = self.dense_2[i](self.dense_1[i](ffn_in))\n",
    "            ffn_out = self.ffn_dropout[i](ffn_out, training=training)\n",
    "            ffn_out = ffn_out + ffn_in\n",
    "            ffn_out = self.ffn_norm[i](ffn_out)\n",
    "\n",
    "            bot_sub_in = ffn_out\n",
    "\n",
    "        logits = self.dense(ffn_out)\n",
    "\n",
    "        return logits, bot_alignments, mid_alignments\n",
    "\n",
    "\n",
    "vocab_size = len(fr_tokenizer.word_index) + 1\n",
    "decoder = Decoder(vocab_size, MODEL_SIZE, NUM_LAYERS, H)\n",
    "\n",
    "sequence_in = tf.constant([[14, 24, 36, 0, 0]])\n",
    "decoder_output, _, _ = decoder(sequence_in, encoder_output)\n",
    "decoder_output.shape\n",
    "\n",
    "\n",
    "crossentropy = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True)\n",
    "\n",
    "\n",
    "def loss_func(targets, logits):\n",
    "    mask = tf.math.logical_not(tf.math.equal(targets, 0))\n",
    "    mask = tf.cast(mask, dtype=tf.int64)\n",
    "    loss = crossentropy(targets, logits, sample_weight=mask)\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "class WarmupThenDecaySchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \"\"\" Learning schedule for training the Transformer\n",
    "\n",
    "    Attributes:\n",
    "        model_size: d_model in the paper (depth size of the model)\n",
    "        warmup_steps: number of warmup steps at the beginning\n",
    "    \"\"\"\n",
    "    def __init__(self, model_size, warmup_steps=4000):\n",
    "        super(WarmupThenDecaySchedule, self).__init__()\n",
    "\n",
    "        self.model_size = model_size\n",
    "        self.model_size = tf.cast(self.model_size, tf.float32)\n",
    "\n",
    "        self.warmup_steps = warmup_steps\n",
    "\n",
    "    def __call__(self, step):\n",
    "        step_term = tf.math.rsqrt(step)\n",
    "        warmup_term = step * (self.warmup_steps ** -1.5)\n",
    "\n",
    "        return tf.math.rsqrt(self.model_size) * tf.math.minimum(step_term, warmup_term)\n",
    "\n",
    "\n",
    "lr = WarmupThenDecaySchedule(MODEL_SIZE)\n",
    "optimizer = tf.keras.optimizers.Adam(lr,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98,\n",
    "                                     epsilon=1e-9)\n",
    "\n",
    "\n",
    "def predict(test_source_text=None):\n",
    "    \"\"\" Predict the output sentence for a given input sentence\n",
    "\n",
    "    Args:\n",
    "        test_source_text: input sentence (raw string)\n",
    "    \n",
    "    Returns:\n",
    "        The encoder's attention vectors\n",
    "        The decoder's bottom attention vectors\n",
    "        The decoder's middle attention vectors\n",
    "        The input string array (input sentence split by ' ')\n",
    "        The output string array\n",
    "    \"\"\"\n",
    "    if test_source_text is None:\n",
    "        test_source_text = raw_data_en[np.random.choice(len(raw_data_en))]\n",
    "    print(test_source_text)\n",
    "    test_source_seq = en_tokenizer.texts_to_sequences([test_source_text])\n",
    "    print(test_source_seq)\n",
    "\n",
    "    en_output, en_alignments = encoder(tf.constant(test_source_seq), training=False)\n",
    "\n",
    "    de_input = tf.constant(\n",
    "        [[fr_tokenizer.word_index['<start>']]], dtype=tf.int64)\n",
    "\n",
    "    out_words = []\n",
    "\n",
    "    while True:\n",
    "        de_output, de_bot_alignments, de_mid_alignments = decoder(de_input, en_output, training=False)\n",
    "        new_word = tf.expand_dims(tf.argmax(de_output, -1)[:, -1], axis=1)\n",
    "        out_words.append(fr_tokenizer.index_word[new_word.numpy()[0][0]])\n",
    "\n",
    "        # Transformer doesn't have sequential mechanism (i.e. states)\n",
    "        # so we have to add the last predicted word to create a new input sequence\n",
    "        de_input = tf.concat((de_input, new_word), axis=-1)\n",
    "\n",
    "        # TODO: get a nicer constraint for the sequence length!\n",
    "        if out_words[-1] == '<end>' or len(out_words) >= 14:\n",
    "            break\n",
    "\n",
    "    print(' '.join(out_words))\n",
    "    return en_alignments, de_bot_alignments, de_mid_alignments, test_source_text.split(' '), out_words\n",
    "\n",
    "\n",
    "@tf.function\n",
    "def train_step(source_seq, target_seq_in, target_seq_out):\n",
    "    \"\"\" Execute one training step (forward pass + backward pass)\n",
    "\n",
    "    Args:\n",
    "        source_seq: source sequences\n",
    "        target_seq_in: input target sequences (<start> + ...)\n",
    "        target_seq_out: output target sequences (... + <end>)\n",
    "    \n",
    "    Returns:\n",
    "        The loss value of the current pass\n",
    "    \"\"\"\n",
    "    with tf.GradientTape() as tape:\n",
    "        encoder_mask = 1 - tf.cast(tf.equal(source_seq, 0), dtype=tf.float32)\n",
    "        # encoder_mask has shape (batch_size, source_len)\n",
    "        # we need to add two more dimensions in between\n",
    "        # to make it broadcastable when computing attention heads\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_mask = tf.expand_dims(encoder_mask, axis=1)\n",
    "        encoder_output, _ = encoder(source_seq, encoder_mask=encoder_mask)\n",
    "\n",
    "        decoder_output, _, _ = decoder(\n",
    "            target_seq_in, encoder_output, encoder_mask=encoder_mask)\n",
    "\n",
    "        loss = loss_func(target_seq_out, decoder_output)\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return loss\n",
    "\n",
    "\n",
    "NUM_EPOCHS = 2\n",
    "\n",
    "starttime = time.time()\n",
    "for e in range(NUM_EPOCHS):\n",
    "    for batch, (source_seq, target_seq_in, target_seq_out) in enumerate(dataset.take(-1)):\n",
    "        loss = train_step(source_seq, target_seq_in,\n",
    "                          target_seq_out)\n",
    "        if batch % 100 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Elapsed time {:.2f}s'.format(\n",
    "                e + 1, batch, loss.numpy(), time.time() - starttime))\n",
    "            starttime = time.time()\n",
    "\n",
    "    try:\n",
    "        predict()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "test_sents = (\n",
    "    'What a ridiculous concept!',\n",
    "    'Your idea is not entirely crazy.',\n",
    "    \"A man's worth lies in what he is.\",\n",
    "    'What he did is very wrong.',\n",
    "    \"All three of you need to do that.\",\n",
    "    \"Are you giving me another chance?\",\n",
    "    \"Both Tom and Mary work as models.\",\n",
    "    \"Can I have a few minutes, please?\",\n",
    "    \"Could you close the door, please?\",\n",
    "    \"Did you plant pumpkins this year?\",\n",
    "    \"Do you ever study in the library?\",\n",
    "    \"Don't be deceived by appearances.\",\n",
    "    \"Excuse me. Can you speak English?\",\n",
    "    \"Few people know the true meaning.\",\n",
    "    \"Germany produced many scientists.\",\n",
    "    \"Guess whose birthday it is today.\",\n",
    "    \"He acted like he owned the place.\",\n",
    "    \"Honesty will pay in the long run.\",\n",
    "    \"How do we know this isn't a trap?\",\n",
    "    \"I can't believe you're giving up.\",\n",
    ")\n",
    "\n",
    "for i, test_sent in enumerate(test_sents):\n",
    "    test_sequence = normalize_string(test_sent)\n",
    "    predict(test_sequence)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
