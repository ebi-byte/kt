{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.\n",
    "Naive Bayes is a machine learning implementation of Bayes Theorem.  It is a classification algorithm that predicts the probability of each data point belonging to a class and then classifies the point as the class with the highest probability.\n",
    "It is naive because while it uses conditional probability to make classifications, the algorithm simply assumes that all features of a class are independent and all make equal contributions to the classification process.  This is considered naive because, in reality, it is not often the case.  The upside is that the math is simpler, the classifier runs quicker, and the results are often quite good for certain problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a dataset\n",
    "import pandas as pd \n",
    "from sklearn.feature_extraction.text import CountVectorizer \n",
    "\n",
    "dataset = [[\"I liked the movie\", \"positive\"], [\"It’s a good movie. Nice story\", \"positive\"], [\"Hero’s acting is bad but heroine looks good. Overall nice movie\", \"positive\"], [\"Nice songs. But sadly boring ending.\", \"negative\"], [\"sad movie, boring movie\", \"negative\"]] \n",
    "dataset = pd.DataFrame(dataset) \n",
    "dataset.columns = [\"Text\", \"Reviews\"] \n",
    "x = dataset[\"Text\"]\n",
    "\n",
    "# creating bag of words model \n",
    "cv = CountVectorizer(max_features = 1500) \n",
    "\n",
    "X = cv.fit_transform(x).toarray() \n",
    "y = dataset.iloc[:, 1].values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "# splitting the data set into training set and test set \n",
    "from sklearn.model_selection import train_test_split \n",
    "  \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0) \n",
    "\n",
    "# fitting multinomial naive bayes to the training set \n",
    "from sklearn.naive_bayes import MultinomialNB \n",
    "from sklearn.metrics import confusion_matrix \n",
    "  \n",
    "classifier = MultinomialNB(); \n",
    "classifier.fit(X_train, y_train) \n",
    "  \n",
    "# predicting test set results \n",
    "y_pred = classifier.predict(X_test) \n",
    "\n",
    "print(classifier.score(X_test,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The score observed is pretty low as a major part of data cleaning is skipped as it is out of the scope of current discussion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
