{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Rule Based Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rule-based classifier makes use of a set of IF-THEN rules for classification. We can express a rule in the following from − IF condition THEN conclusion. \n",
    "\n",
    "Let us consider a rule R1, R1: IF age = youth AND student = yes, THEN buy_computer = yes. \n",
    "\n",
    "The IF part of the rule is called rule condition and the THEN part of the rule is called rule consequent. The condition part consists of one or more attribute tests and these tests are logically related (AND, OR). The consequent part consists of class prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.1. One Rule Classification"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Consider the following dataset example.\n",
    "\n",
    "**Name**  **Blood_type**  **Give_birth**  **Can_Fly**  ** Live_in_water**  **Class** \n",
    "human         warm             yes            no               no           mammals\n",
    "python        cold              no            no               no          reptiles\n",
    "pigeon        warm              no           yes               no            birds\n",
    "eagle         warm              no           yes               no            birds\n",
    "whale         warm             yes            no              yes           mammals\n",
    "frog          cold              no            no            sometimes     amphibians\n",
    "salmon        cold              no            no              yes           fishes "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate a large number of rules with the data and check the error rates and pick the set of rules with the least error rates. \n",
    "\n",
    "Here are a few rules.\n",
    "\n",
    "    R1:(Give Birth = no) →Birds\n",
    "    R2:(Can Fly = no) →Reptiles\n",
    "    R3:(Give Birth = no) ∧ (Can Fly = yes) →Birds \n",
    "    R4:(Give Birth = yes) ∧ (Blood Type = warm) →Mammals \n",
    "    R5:(Give Birth = no) ∧ (Can Fly = no) →Reptiles \n",
    "    R6:(Give Birth = no) ∧(Live in Water = yes) →Fishes\n",
    "\n",
    "Observe the above set of rules. The rules R1 and R2 contain only one variable while R3, R4, R5 have 2 variables. \n",
    "\n",
    "Rule R1 has an error rate of 33% as it would classify python as a bird, rule R2 has an error rate of 25% as it will misclassify mammals and amphibians as reptiles. Rule R5 has an error rate of 50% as it would classify amphibians as reptiles, instead we can modify the rule as \n",
    "    \n",
    "    R5:(Give Birth = no) ∧ (Can Fly = no) ∧ (Live in water = no) →Reptiles\n",
    "\n",
    "This will give an error rate of 0% with this sample.\n",
    "\n",
    "However, rules R3, R4, R6 have 0 error rates. It is because they took multiple features into consideration and therefore, can produce more accurate results. However, there is also a disadvantage in using a lot of features in a rule. It will increase the specificity of the rule and therefore an incoming datapoint maynot fall under any rule and therefore lead to the unstability of the classifier. \n",
    "\n",
    "For example, consider leopard shark. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "**Name**  **Blood_type**  **Give_birth**  **Can_Fly**  ** Live_in_water**  **Class** \n",
    "leopardshark  cold              yes            no             yes            fishes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though leopard shark is a fish, it can't be validated with the help of any of the rules created and therefore can't be classified."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.2. Rule Extraction\n",
    "The rule extraction from a decision tree is pretty straight forward. The leaf nodes contain all the classes and hence is the consequent part of the rule. Every path from the root node leading to the leaf node is a rule whose corresponding consequent part is stored in that leaf node.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.3. Strategies for Learning Rules\n",
    "##### General-to-Specific \n",
    "Start with an empty rule. Add constraints to eliminate negative examples. Stop when only positive examples are covered.\n",
    "\n",
    "##### Specific-to-General \n",
    "Start with a rule that identifies a single random instance. Remove constraints to cover more positive examples. Stop when further generalization starts covering negatives."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.4. Rule Pruning\n",
    "The Assessment of quality is made on the original set of training data. The rule may perform well on training data but less well on subsequent data. That's why the rule pruning is required. The rule is pruned by altering the conjunct. The rule R is pruned, if pruned version of R has greater quality when assessed on an independent set of tuples(cross validation sets). \n",
    "\n",
    "FOIL is one of the simplest and effective method for rule pruning. For a given rule R,\n",
    "\n",
    "FOIL_Prune = pos - neg / pos + neg\n",
    "\n",
    "where pos and neg is the number of positive and negative tuples covered by R, respectively. Postive tuples are those correctly predicted and negative are those incorrectly predicted. This value will increase with the accuracy of R on the pruning set. Hence, if the FOIL_Prune value is higher for the pruned version of R, then we prune R."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1.1.5. Pros and Cons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The main advantage is ease of interpretation (as long as there aren’t too many rules) - basically a human can understand how the model makes predictions & whether it makes sense. For a specific instance it is possible to verify that the process worked correctly, and see what the main factors in the prediction were.\n",
    "\n",
    "The main disadvantage is that rule-based methods are usually not the best performers in terms of prediction quality. Other methods (forests, SVM, deep nets) tends to be better. Also, rule-based methods are better only for data with categorical features. Covering all the possibilities is a very difficult task especially if you are working with a large set of featured data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. Make a rule based classification for the following data**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Age       Spectacle_prescription  Astigmatism  Tear_production_rate   Recommended_Lenses\n",
    "young              myope              no            reduced                  none \n",
    "young              myope              no            normal                   soft \n",
    "young              myope             yes            reduced                  none \n",
    "young              myope             yes            normal                   hard \n",
    "young            hypermetrope         no            reduced                  none \n",
    "young            hypermetrope         no            normal                   soft \n",
    "young            hypermetrope        yes            reduced                  none \n",
    "young            hypermetrope        yes            normal                   hard \n",
    "pre-presbyopic     myope              no            reduced                  none \n",
    "pre-presbyopic     myope              no            normal                   soft \n",
    "pre-presbyopic     myope              no            normal                   soft \n",
    "pre-presbyopic     myope             yes            reduced                  none \n",
    "pre-presbyopic     myope             yes            normal                   hard \n",
    "pre-presbyopic   hypermetrope         no            reduced                  none \n",
    "pre-presbyopic   hypermetrope         no            normal                   soft \n",
    "pre-presbyopic   hypermetrope        yes            reduced                  none \n",
    "pre-presbyopic   hypermetrope        yes            normal                   none \n",
    "presbyopic         myope              no            reduced                  none \n",
    "presbyopic         myope              no            normal                   none \n",
    "presbyopic         myope             yes            reduced                  none \n",
    "presbyopic         myope             yes            normal                   hard \n",
    "presbyopic       hypermetrope         no            reduced                  none \n",
    "presbyopic       hypermetrope         no            normal                   soft \n",
    "presbyopic       hypermetrope        yes            reduced                  none \n",
    "presbyopic       hypermetrope        yes            normal                   none"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Solutions](https://github.com/ebi-byte/kt/blob/master/supervised_ML/RBC%20Solution.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
