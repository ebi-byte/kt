{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bagging"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea behind bagging is combining the results of multiple models (for instance, all decision trees) to get a generalized result.  \n",
    "If we create all the models on the same set of data and combine it, it will not be useful as there is a high chance that these models will give the same result since they are getting the same input. One of the technique to overcome this problem is bootstrapping.\n",
    "\n",
    "Bootstrapping is a sampling technique in which we create subsets of observations from the original dataset, with replacement. The size of the subsets is the same as the size of the original set.\n",
    "\n",
    "Bagging (or Bootstrap Aggregating) technique uses these subsets (bags) to get a fair idea of the distribution (complete set). The size of subsets created for bagging may be less than the original set.  \n",
    "1.Multiple subsets are created from the original dataset, selecting observations with replacement.   \n",
    "2.A base model (weak model) is created on each of these subsets.  \n",
    "3.The models run in parallel and are independent of each other.  \n",
    "4.The final predictions are determined by combining the predictions from all the models.  \n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/Screenshot-from-2018-05-08-13-11-49.png\" alt=\"Drawing\" style=\"height: 350px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Boosting is a sequential process, where each subsequent model attempts to correct the errors of the previous model. The succeeding models are dependent on the previous model. Let’s understand the way boosting works in the below steps.\n",
    "1.A subset is created from the original dataset.\n",
    "2.Initially, all data points are given equal weights.\n",
    "3.A base model is created on this subset.\n",
    "4.This model is used to make predictions on the whole dataset.\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2015/11/dd1-e1526989432375.png\" alt=\"Drawing\" style=\"height: 150px;\"/>\n",
    "5.Errors are calculated using the actual values and predicted values.\n",
    "6.The observations which are incorrectly predicted, are given higher weights.\n",
    " (Here, the three misclassified blue-plus points will be given higher weights)\n",
    "7.Another model is created and predictions are made on the dataset.\n",
    " (This model tries to correct the errors from the previous model)\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2015/11/dd2-e1526989487878.png\" alt=\"Drawing\" style=\"height: 150px;\"/>\n",
    "8.Similarly, multiple models are created, each correcting the errors of the previous model.  \n",
    "9.The final model (strong learner) is the weighted mean of all the models (weak learners).\n",
    "\n",
    "Thus, the boosting algorithm combines a number of weak learners to form a strong learner. The individual models would not perform well on the entire dataset, but they work well for some part of the dataset. Thus, each model actually boosts the performance of the ensemble.\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2015/11/dd4-e1526551014644.png\" alt=\"Drawing\" style=\"height: 150px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithms based on Bagging and Boosting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bagging algorithm:\n",
    "•Random forest\n",
    "\n",
    "Random Forest is an ensemble machine learning algorithm that follows the bagging technique. The base estimators in random forest are decision trees. Random forest randomly selects a set of features which are used to decide the best split at each node of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Boosting algorithms:\n",
    "•Ada Boost  \n",
    "•Gradient boosting  \n",
    "•XG boost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Ada Boost\n",
    "Adaptive boosting or AdaBoost is one of the simplest boosting algorithms. Usually, decision trees are used for modelling. Multiple sequential models are created, each correcting the errors from the last model. AdaBoost assigns weights to the observations which are incorrectly predicted and the subsequent model works to predict these values correctly.\n",
    "\n",
    "Below are the steps for performing the AdaBoost algorithm:  \n",
    "1.Initially, all observations in the dataset are given equal weights.  \n",
    "2.A model is built on a subset of data.  \n",
    "3.Using this model, predictions are made on the whole dataset.  \n",
    "4.Errors are calculated by comparing the predictions and actual values.  \n",
    "5.While creating the next model, higher weights are given to the data points which were predicted incorrectly.  \n",
    "6.Weights can be determined using the error value. For instance, higher the error more is the weight assigned to the observation.  \n",
    "7.This process is repeated until the error function does not change, or the maximum limit of the number of estimators is reached."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None)\n",
    "\n",
    "#split dataset in features and target variable\n",
    "X = pima.iloc[:, 0:8].values\n",
    "y = pima.iloc[:, 8].values\n",
    "\n",
    "#Test-train split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7489177489177489"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Ada Boost\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "model = AdaBoostClassifier(random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Gradient Boosting (GBM)\n",
    "Gradient Boosting or GBM is another ensemble machine learning algorithm that works for both regression and classification problems. GBM uses the boosting technique, combining a number of weak learners to form a strong learner. Regression trees used as a base learner, each subsequent tree in series is built on the errors calculated by the previous tree.\n",
    "\n",
    "We will use a simple example to understand the GBM algorithm. We have to predict the age of a group of people using the below data:\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/image-17.png\" alt=\"Drawing\" style=\"height: 150px;\"/>\n",
    "1.The mean age is assumed to be the predicted value for all observations in the dataset.  \n",
    "2.The errors are calculated using this mean prediction and actual values of age.\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/05/image-18.png\" alt=\"Drawing\" style=\"height: 150px;\"/>\n",
    "3.A tree model is created using the errors calculated above as target variable. Our objective is to find the best split to minimize the error.  \n",
    "4.The predictions by this model are combined with the predictions 1.\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/gbm2.png\" alt=\"Drawing\" style=\"height: 150px;\"/>\n",
    "5.This value calculated above is the new prediction.  \n",
    "6.New errors are calculated using this predicted value and actual value.\n",
    "<img src=\"https://s3-ap-south-1.amazonaws.com/av-blog-media/wp-content/uploads/2018/06/gbm3.png\" alt=\"Drawing\" style=\"height: 120px;\"/>\n",
    "7.Steps 2 to 6 are repeated till the maximum number of iterations is reached (or error function does not change).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7662337662337663"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#GBM\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "model= GradientBoostingClassifier(learning_rate=0.01,random_state=1)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "XGBoost (extreme Gradient Boosting) is an advanced implementation of the gradient boosting algorithm. XGBoost has proved to be a highly effective ML algorithm, extensively used in machine learning competitions and hackathons. XGBoost has high predictive power and is almost 10 times faster than the other gradient boosting techniques. It also includes a variety of regularization which reduces overfitting and improves overall performance. Hence it is also known as **‘regularized boosting‘** technique.\n",
    "\n",
    "Let us see how XGBoost is comparatively better than other techniques:  \n",
    "1.**_Regularization_**:  \n",
    "◦Standard GBM implementation has no regularisation like XGBoost.  \n",
    "◦Thus XGBoost also helps to reduce overfitting.\n",
    "\n",
    "2.**_Parallel Processing_**:  \n",
    "◦XGBoost implements parallel processing and is faster than GBM .  \n",
    "◦XGBoost also supports implementation on Hadoop.\n",
    "\n",
    "3.**_High Flexibility_**:  \n",
    "◦XGBoost allows users to define custom optimization objectives and evaluation criteria adding a whole new dimension to the model.\n",
    "\n",
    "4.**_Handling Missing Values_**:  \n",
    "◦XGBoost has an in-built routine to handle missing values.\n",
    "\n",
    "5.**_Tree Pruning_**:  \n",
    "◦XGBoost makes splits up to the max_depth specified and then starts pruning the tree backwards and removes splits beyond which there is no positive gain.\n",
    "\n",
    "6.**_Built-in Cross-Validation_**:  \n",
    "◦XGBoost allows a user to run a cross-validation at each iteration of the boosting process and thus it is easy to get the exact optimum number of boosting iterations in a single run."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#XG Boost\n",
    "import xgboost as xgb\n",
    "model=xgb.XGBClassifier(random_state=1,learning_rate=0.01)\n",
    "model.fit(X_train, y_train)\n",
    "model.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are ensemble methods in tree based modeling ?\n",
    "##### How do Bagging and Boosting get N learners?\n",
    "##### Which is the best, Bagging or Boosting?\n",
    "\n",
    "[Solution](https://github.com/ebi-byte/kt/blob/master/trees/Trees%20Questionnaire.ipynb\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
