{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is difference between Pre-pruning and Post-pruning of decision tree?\n",
    "\n",
    "**_Post-pruning_** is also known as backward pruning. In this, first Generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the classification accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately.\n",
    "\n",
    "**_Pre-pruning_** is also called forward pruning or online-pruning. Pre-pruning prevent the generation of non-significant branches. Pre-pruning a decision tree involves using a ‘termination condition’ to decide when it is desirable to terminate some of the branches prematurely as the tree is generated. When constructing the tree some significant measures can be used to assess the goodness of a split. If partitioning the tuples at a node would result the split that falls below a prespecified threshold, then further partitioning of the given subset is halted otherwise it is expanded. High threshold result in oversimplified trees, whereas low threshold result in very little simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is over fitting in decision tree?\n",
    "\n",
    "Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data.\n",
    "\n",
    "In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus it ends up with branches with strict rules of sparse data. Thus this effects the accuracy when predicting samples that are not part of the training set.\n",
    "\n",
    "One of the methods used to address over-fitting in decision tree is called pruning which is done after the initial training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are tree based models better than linear models?\n",
    "\n",
    "It is dependent on the type of problem we are solving. Let’s look at some key factors which will help us to decide which algorithm to use:  \n",
    "1.If the relationship between dependent & independent variable is well approximated by a linear model, linear regression will outperform tree based model.  \n",
    "2.If there is a high non-linearity & complex relationship between dependent & independent variables, a tree model will outperform a classical regression method.  \n",
    "3.If we need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are ensemble methods in tree based modeling ?\n",
    "\n",
    "The literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.\n",
    "\n",
    "Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’.\n",
    "\n",
    "We build a small tree and we will get a model with low variance and high bias. Normally, as we increase the complexity of our model, we will see a reduction in prediction error due to lower bias in the model. As we continue to make our model more complex, we end up over-fitting our model and our model will start suffering from high variance.\n",
    "\n",
    "A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis.<img src=\"https://www.dataquest.io/wp-content/uploads/2019/01/biasvariance.png\" alt=\"Drawing\" style=\"height: 200px;\"/>\n",
    "Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Which is the best, Bagging or Boosting?\n",
    "\n",
    "It depends on the data, the simulation and the circumstances.\n",
    "Bagging and Boosting decrease the variance of our single estimate as they combine several estimates from different models. So the result may be a model with higher stability.\n",
    "\n",
    "If the problem is that the single model gets a very low performance, Bagging will rarely get a better bias. However, Boosting could generate a combined model with lower errors as it optimises the advantages and reduces pitfalls of the single model.\n",
    "\n",
    "By contrast, if the difficulty of the single model is over-fitting, then Bagging is the best option. Boosting for its part doesn’t help to avoid over-fitting; in fact, this technique is faced with this problem itself. For this reason, Bagging is effective more often than Boosting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### How do Bagging and Boosting get N learners?\n",
    "\n",
    "Bagging and Boosting get N learners by generating additional data in the training stage. N new training data sets are produced by random sampling with replacement from the original set. By sampling with replacement some observations may be repeated in each new training data set.\n",
    "\n",
    "In the case of Bagging, any element has the same probability to appear in a new data set. However, for Boosting the observations are weighted and therefore some of them will take part in the new sets more often.\n",
    "\n",
    "These multiple sets are used to train the same learner algorithm and therefore different classifiers are produced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do Random Forest overfit?\n",
    "\n",
    "A Random Forest with few trees is quite prone to overfit to noise. This is easily demonstrated because Random Forest with just one tree is the same as a single tree. As more trees are added, the tendency to overfit generally decreases. It never, however, approaches zero. No number of trees will ever remove overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Show effect of Random Forest hyperparameter tuning  on performance in python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)\n",
    "pima.head()\n",
    "\n",
    "#split dataset in features and target variable\n",
    "X = pima.iloc[:, 0:8].values #features variable\n",
    "y = pima.iloc[:, 8].values   #target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "# 70% training and 30% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)\n",
    "\n",
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_No Hyperparameter tuning_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7575757575757576\n",
      "[[137  20]\n",
      " [ 36  38]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**__n_estimators__**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n",
      "[[140  17]\n",
      " [ 34  40]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increasing n_estimators from 20 to 100 increases accuracy from 75.76% to 77.92%\n",
    "\n",
    "**_criterion_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7792207792207793\n",
      "[[139  18]\n",
      " [ 33  41]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20 , criterion='entropy', random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Changing criterion from gini to entropy changes accuracy from 75.76% to 77.92%\n",
    "\n",
    "**_max_depth_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7748917748917749\n",
      "[[142  15]\n",
      " [ 37  37]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20 , max_depth=5, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Restricting depth of tree to 5 changes accuracy from 75.76% to 77.49%\n",
    "\n",
    "**_min_samples_split_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8095238095238095\n",
      "[[142  15]\n",
      " [ 29  45]]\n"
     ]
    }
   ],
   "source": [
    "classifier = RandomForestClassifier(n_estimators=20 , min_samples_split=5, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keeping min_samples_split = 5 increases accuracy from 75.76% to 80.95%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
