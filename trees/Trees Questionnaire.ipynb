{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is difference between Pre-pruning and Post-pruning of decision tree?\n",
    "\n",
    "**_Post-pruning_** is also known as backward pruning. In this, first Generate the decision tree and then remove non-significant branches. Post-pruning a decision tree implies that we begin by generating the (complete) tree and then adjust it with the aim of improving the classification accuracy on unseen instances. There are two principal methods of doing this. One method that is widely used begins by converting the tree to an equivalent set of rules. Another commonly used approach aims to retain the decision tree but to replace some of its subtrees by leaf nodes, thus converting a complete tree to a smaller pruned one which predicts the classification of unseen instances at least as accurately.\n",
    "\n",
    "**_Pre-pruning_** is also called forward pruning or online-pruning. Pre-pruning prevent the generation of non-significant branches. Pre-pruning a decision tree involves using a ‘termination condition’ to decide when it is desirable to terminate some of the branches prematurely as the tree is generated. When constructing the tree some significant measures can be used to assess the goodness of a split. If partitioning the tuples at a node would result the split that falls below a prespecified threshold, then further partitioning of the given subset is halted otherwise it is expanded. High threshold result in oversimplified trees, whereas low threshold result in very little simplification."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What is over fitting in decision tree?\n",
    "\n",
    "Over-fitting is the phenomenon in which the learning system tightly fits the given training data so much that it would be inaccurate in predicting the outcomes of the untrained data.\n",
    "\n",
    "In decision trees, over-fitting occurs when the tree is designed so as to perfectly fit all samples in the training data set. Thus it ends up with branches with strict rules of sparse data. Thus this effects the accuracy when predicting samples that are not part of the training set.\n",
    "\n",
    "One of the methods used to address over-fitting in decision tree is called pruning which is done after the initial training is complete."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Are tree based models better than linear models?\n",
    "\n",
    "It is dependent on the type of problem we are solving. Let’s look at some key factors which will help us to decide which algorithm to use:  \n",
    "1.If the relationship between dependent & independent variable is well approximated by a linear model, linear regression will outperform tree based model.  \n",
    "2.If there is a high non-linearity & complex relationship between dependent & independent variables, a tree model will outperform a classical regression method.  \n",
    "3.If we need to build a model which is easy to explain to people, a decision tree model will always do better than a linear model. Decision tree models are even simpler to interpret than linear regression."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What are ensemble methods in tree based modeling ?\n",
    "\n",
    "The literary meaning of word ‘ensemble’ is group. Ensemble methods involve group of predictive models to achieve a better accuracy and model stability. Ensemble methods are known to impart supreme boost to tree based models.\n",
    "\n",
    "Like every other model, a tree based model also suffers from the plague of bias and variance. Bias means, ‘how much on an average are the predicted values different from the actual value.’ Variance means, ‘how different will the predictions of the model be at the same point if different samples are taken from the same population’.\n",
    "\n",
    "We build a small tree and we will get a model with low variance and high bias. Normally, as we increase the complexity of our model, we will see a reduction in prediction error due to lower bias in the model. As we continue to make our model more complex, we end up over-fitting our model and our model will start suffering from high variance.\n",
    "\n",
    "A champion model should maintain a balance between these two types of errors. This is known as the trade-off management of bias-variance errors. Ensemble learning is one way to execute this trade off analysis.<img src=\"https://www.dataquest.io/wp-content/uploads/2019/01/biasvariance.png\" alt=\"Drawing\" style=\"height: 200px;\"/>\n",
    "Some of the commonly used ensemble methods include: Bagging, Boosting and Stacking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
