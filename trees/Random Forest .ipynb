{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest is a type of supervised machine learning algorithm based on ensemble learning. Ensemble learning is a type of learning where we join different types of algorithms or same algorithm multiple times to form a more powerful prediction model. The random forest algorithm combines multiple algorithm of the same type i.e. multiple decision trees, resulting in a forest of trees, hence the name \"Random Forest\". The random forest algorithm can be used for both regression and classification tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest Algorithm "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are the basic steps involved in performing the random forest algorithm:  \n",
    "1.Pick N random records from the dataset.  \n",
    "2.Build a decision tree based on these N records.    \n",
    "3.Choose the number of trees you want in your algorithm and repeat steps 1 and 2.  \n",
    "4.In case of a regression problem, for a new record, each tree in the forest predicts a value for Y (output). The final value can be calculated by taking the average of all the values predicted by all the trees in forest. Or, in case of a classification problem, each tree in the forest predicts the category to which the new record belongs. Finally, the new record is assigned to the category that wins the majority vote.<img src=\"http://res.cloudinary.com/dyd911kmh/image/upload/f_auto,q_auto:best/v1526467744/voting_dnjweq.jpg\" alt=\"Drawing\" style=\"height: 300px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest for Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to predict the gas consumption (in millions of gallons) in 48 of the US states based on petrol tax (in cents), income (dollars) and the proportion of population with the driving license."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Petrol_tax</th>\n",
       "      <th>Income</th>\n",
       "      <th>Prop_drivers_licenses</th>\n",
       "      <th>Consum_mill_gallons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>7.918</td>\n",
       "      <td>3571</td>\n",
       "      <td>0.551</td>\n",
       "      <td>541</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>7.952</td>\n",
       "      <td>4092</td>\n",
       "      <td>0.534</td>\n",
       "      <td>524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>7.878</td>\n",
       "      <td>3865</td>\n",
       "      <td>0.563</td>\n",
       "      <td>561</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>6.672</td>\n",
       "      <td>4870</td>\n",
       "      <td>0.415</td>\n",
       "      <td>414</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>7.180</td>\n",
       "      <td>4399</td>\n",
       "      <td>0.416</td>\n",
       "      <td>410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   City  Petrol_tax  Income  Prop_drivers_licenses  Consum_mill_gallons\n",
       "0     1       7.918    3571                  0.551                  541\n",
       "1     2       7.952    4092                  0.534                  524\n",
       "2     3       7.878    3865                  0.563                  561\n",
       "3     4       6.672    4870                  0.415                  414\n",
       "4     5       7.180    4399                  0.416                  410"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv('Petrol_consumption.csv')\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Petrol_tax</th>\n",
       "      <th>Income</th>\n",
       "      <th>Prop_drivers_licenses</th>\n",
       "      <th>Consum_mill_gallons</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>48.00</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.000000</td>\n",
       "      <td>48.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>24.50</td>\n",
       "      <td>6.400208</td>\n",
       "      <td>4241.833333</td>\n",
       "      <td>0.632229</td>\n",
       "      <td>576.770833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>14.00</td>\n",
       "      <td>1.463582</td>\n",
       "      <td>573.623768</td>\n",
       "      <td>0.146151</td>\n",
       "      <td>111.885816</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.00</td>\n",
       "      <td>2.572000</td>\n",
       "      <td>3063.000000</td>\n",
       "      <td>0.352000</td>\n",
       "      <td>344.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>12.75</td>\n",
       "      <td>5.769000</td>\n",
       "      <td>3739.000000</td>\n",
       "      <td>0.524500</td>\n",
       "      <td>509.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>24.50</td>\n",
       "      <td>6.756000</td>\n",
       "      <td>4298.000000</td>\n",
       "      <td>0.609500</td>\n",
       "      <td>568.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>36.25</td>\n",
       "      <td>7.389500</td>\n",
       "      <td>4578.750000</td>\n",
       "      <td>0.744000</td>\n",
       "      <td>632.750000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>48.00</td>\n",
       "      <td>9.086000</td>\n",
       "      <td>5342.000000</td>\n",
       "      <td>0.930000</td>\n",
       "      <td>968.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        City  Petrol_tax       Income  Prop_drivers_licenses  \\\n",
       "count  48.00   48.000000    48.000000              48.000000   \n",
       "mean   24.50    6.400208  4241.833333               0.632229   \n",
       "std    14.00    1.463582   573.623768               0.146151   \n",
       "min     1.00    2.572000  3063.000000               0.352000   \n",
       "25%    12.75    5.769000  3739.000000               0.524500   \n",
       "50%    24.50    6.756000  4298.000000               0.609500   \n",
       "75%    36.25    7.389500  4578.750000               0.744000   \n",
       "max    48.00    9.086000  5342.000000               0.930000   \n",
       "\n",
       "       Consum_mill_gallons  \n",
       "count            48.000000  \n",
       "mean            576.770833  \n",
       "std             111.885816  \n",
       "min             344.000000  \n",
       "25%             509.500000  \n",
       "50%             568.500000  \n",
       "75%             632.750000  \n",
       "max             968.000000  "
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first task is to divide data into 'attributes' and 'label' sets. The resultant data is then divided into training and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.iloc[:, 1:4].values\n",
    "y = dataset.iloc[:, 4].values\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have scaled our dataset, it is time to train our random forest algorithm to solve this regression problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "regressor = RandomForestRegressor(n_estimators=20, random_state=0)\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RandomForestRegressor class of the sklearn.ensemble library is used to solve regression problems via random forest. The most important parameter of the RandomForestRegressor class is the n_estimators parameter. This parameter defines the number of trees in the random forest. We will start with n_estimator=20 to see how our algorithm performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 42.443333333333335\n",
      "Mean Squared Error: 3184.298833333333\n",
      "Root Mean Squared Error: 56.4295918232033\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))\n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))\n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 20 trees, the root mean squared error is 56.42 which is about 10 percent of the average petrol consumption i.e. 576.77. This may indicate, among other things, that we have not used enough estimators (trees)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Random Forest for Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first load the Pima Indian Diabetes dataset using pandas' read CSV function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pregnant</th>\n",
       "      <th>glucose</th>\n",
       "      <th>bp</th>\n",
       "      <th>skin</th>\n",
       "      <th>insulin</th>\n",
       "      <th>bmi</th>\n",
       "      <th>pedigree</th>\n",
       "      <th>age</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6</td>\n",
       "      <td>148</td>\n",
       "      <td>72</td>\n",
       "      <td>35</td>\n",
       "      <td>0</td>\n",
       "      <td>33.6</td>\n",
       "      <td>0.627</td>\n",
       "      <td>50</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>85</td>\n",
       "      <td>66</td>\n",
       "      <td>29</td>\n",
       "      <td>0</td>\n",
       "      <td>26.6</td>\n",
       "      <td>0.351</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8</td>\n",
       "      <td>183</td>\n",
       "      <td>64</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>23.3</td>\n",
       "      <td>0.672</td>\n",
       "      <td>32</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>89</td>\n",
       "      <td>66</td>\n",
       "      <td>23</td>\n",
       "      <td>94</td>\n",
       "      <td>28.1</td>\n",
       "      <td>0.167</td>\n",
       "      <td>21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>137</td>\n",
       "      <td>40</td>\n",
       "      <td>35</td>\n",
       "      <td>168</td>\n",
       "      <td>43.1</td>\n",
       "      <td>2.288</td>\n",
       "      <td>33</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   pregnant  glucose  bp  skin  insulin   bmi  pedigree  age  label\n",
       "0         6      148  72    35        0  33.6     0.627   50      1\n",
       "1         1       85  66    29        0  26.6     0.351   31      0\n",
       "2         8      183  64     0        0  23.3     0.672   32      1\n",
       "3         1       89  66    23       94  28.1     0.167   21      0\n",
       "4         0      137  40    35      168  43.1     2.288   33      1"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "col_names = ['pregnant', 'glucose', 'bp', 'skin', 'insulin', 'bmi', 'pedigree', 'age', 'label']\n",
    "# load dataset\n",
    "pima = pd.read_csv(\"pima-indians-diabetes.csv\", header=None, names=col_names)\n",
    "pima.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split dataset in features and target variable\n",
    "X = pima.iloc[:, 0:8].values #features variable\n",
    "y = pima.iloc[:, 8].values   #target variable\n",
    "\n",
    "# Split dataset into training set and test set\n",
    "# 70% training and 30% test\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature Scaling\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have scaled our dataset, we can train our random forests to solve this classification problem.\n",
    "\n",
    "For classification, we will use RandomForestClassifier class of the sklearn.ensemble library. RandomForestClassifier class also takes n_estimators as a parameter. Like before, this parameter defines the number of trees in our random forest. We will start with 20 trees again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "classifier = RandomForestClassifier(n_estimators=20, random_state=0)\n",
    "classifier.fit(X_train, y_train)\n",
    "y_pred = classifier.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For classification problems the metrics used to evaluate an algorithm are accuracy, confusion matrix, precision recall, and F1 values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7532467532467533\n",
      "[[137  20]\n",
      " [ 37  37]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.79      0.87      0.83       157\n",
      "           1       0.65      0.50      0.56        74\n",
      "\n",
      "    accuracy                           0.75       231\n",
      "   macro avg       0.72      0.69      0.70       231\n",
      "weighted avg       0.74      0.75      0.74       231\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "print(confusion_matrix(y_test,y_pred))\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy achieved for by our random forest classifier with 20 trees is 75.32%. Changing the number of estimators can significantly improve the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "Random Forests, however, are more than just bagged trees and use a number of interesting techniques to further decrease correlation between trees and reduce overfitting. A quick look at the documentation for scikit-learn’s implementation of the RandomForestRegressor shows us the hyperparameters we can pass in:\n",
    "\n",
    "_class sklearn.ensemble.RandomForestRegressor(n_estimators=10, criterion=’mse’, max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features=’auto’, max_leaf_nodes=None, min_impurity_decrease=0.0, min_impurity_split=None, bootstrap=True, oob_score=False, n_jobs=1, random_state=None, verbose=0, warm_start=False_\n",
    "\n",
    "Here we take a closer look what _n_estimators, max_depth, min_samples_leaf,_ and _max_features_ do and why each of them could potentially decrease the error of our model.\n",
    "\n",
    "##### n_estimators\n",
    "\n",
    "_n_estimators_ is simply the number of trees. The more uncorrelated trees in our forest, the closer their individual errors get to averaging out. However, more is not always better and here are some considerations to keep in mind:  \n",
    "1.More trees = more computation. Beyond a certain point, the tradeoff may not be worth it.  \n",
    "2.Increasing n_estimators gives diminishing returns.  \n",
    "3.No number of uncorrelated trees will get rid of error caused by the assumptions made by your model or bias resulting from unrepresentative training data.\n",
    "\n",
    "##### max_depth\n",
    "\n",
    "_max_depth_ is the how many splits deep we want each tree to go. _max_depth_ = 50, for example, would limit trees to at most 50 splits down any given branch.  \n",
    "This has the consequence that our Random Forest can no more fit the training data as closely, and is consequently more stable. It has lower variance, giving our model lower error. Remember that even though severely constraining _max_depth_ could increase the bias of each tree given that they may not be able to capture certain patterns in the data before hitting their limit, we need not worry about this. A suitable choice of n_estimators, coupled with bagging, ensures that the bias of the forest as a whole doesn’t increase in the process.\n",
    "\n",
    "##### min_samples_leaf\n",
    "\n",
    "_min_samples_leaf_ tells each tree to stop splitting if doing so would result in the end node of any resulting branch having less than required leaves. _min_samples_leaf_ = 10, for example, tells each tree to stop splitting if doing so would result in the end node of any resulting branch having less than 10 leaves.  \n",
    "\n",
    "##### max_features\n",
    "\n",
    "_max_features_ tells each tree how many features to check when looking for the best split to make. A subtlety here is that passing in _max_features_ = 15 doesn’t mean that each tree picks some subset of 15 features to model. Rather, an individual tree chooses a different random sample of 15 features for each split. Like _min_samples_leaf_, this doesn’t allow a tree to fit too closely to the data. More importantly, the trees in the Random Forest are now even less correlated with one another since they weren’t even trained on the same data. There has been some practical machine learning research showing that forests of less accurate, less correlated trees perform better than forests of more accurate, more correlated trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Important Features Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests also offers a good feature selection indicator. Scikit-learn provides an extra variable with the model, which shows the relative importance or contribution of each feature in the prediction. It automatically computes the relevance score of each feature in the training phase. Then it scales the relevance down so that the sum of all scores is 1.\n",
    "\n",
    "This score will help us choose the most important features and drop the least important ones for model building. \n",
    "\n",
    "Random forest uses gini importance or mean decrease in impurity (MDI) to calculate the importance of each feature. Gini importance is also known as the total decrease in node impurity. This is how much the model fit or accuracy decreases when you drop a variable. The larger the decrease, the more significant the variable is. Here, the mean decrease is a significant parameter for variable selection. The Gini index can describe the overall explanatory power of the variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "glucose     0.235223\n",
       "bmi         0.174842\n",
       "age         0.160977\n",
       "pedigree    0.123695\n",
       "bp          0.080483\n",
       "skin        0.080468\n",
       "pregnant    0.075491\n",
       "insulin     0.068821\n",
       "dtype: float64"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_imp = pd.Series(classifier.feature_importances_ , index = col_names[0:8]).sort_values(ascending=False)\n",
    "feature_imp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the feature importance. Visualizations are easy to understand and interpretable.\n",
    "\n",
    "For visualization, you can use a combination of matplotlib and seaborn:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No handles with labels found to put in legend.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZ8AAAEWCAYAAAC5XZqEAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjAsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+17YcXAAAgAElEQVR4nO3deZwcVbn/8c83BAghMSEBkS1MgggGhACDyiKLcln8ieAVBUQF4UeuCyIqriACIorijiDBKwEBRUARUFkUgyQCYYJZ2LcEiEEJCVkIO3nuH3VaKp2emZqlq2d6vu/Xq19TferUqadOOvPMOVVdpYjAzMysTIMaHYCZmQ08Tj5mZlY6Jx8zMyudk4+ZmZXOycfMzErn5GNmZqVz8rGGkPQzSV+r8z6mSPr/afkISTcW2OZPko6sZ1xm5uRjdSDpBkmn1yg/SNK/JA2OiI9HxDfKiikiLo2IfQvUOyAiLurt/UvaS9L83m63OyS1SApJg3upvU6PTdJkSS9Jejb3OrQX9h2S3tjTdqx8Tj5WD5OBj0hSVflHgEsj4pXyQzKA3ko43fSdiBiWe13ewFgAkLRGo2MYqJx8rB6uBkYB76gUSFoPeA9wcXo/WdIZaXl9SddJWiJpsaRbJQ1K61b5y7Zqu/XSdgslPZOWN60VkKSjJE1Ny1+s+gv8ZUmT07r8VN1RkqZKOju1P1fSAbk2x0r6m6Tlkv4s6aeSLinSQWk/Z0j6e4rhWkmjJV0qaZmkOyW15OqHpOMlPSrpaUnfzfXRIEknS3pM0lOSLpY0Iq2rjHKOkfQ4cDPwt9TskrTvXSRtIelmSYtS+5dKGpnb/zxJJ0qaLWmppMslDZG0LvAnYONcf25cpA9ybW8s6ar07zhX0vG5dW+VdFv6bDwp6RxJa6V1leOYVRlJ5f+dq/rujWl5sqTzJP1R0gpgb0lrp3/jxyX9W9mU8DqpfrufTesZd6L1uoh4HvgN8NFc8QeB+yNiVo1NPg/MBzYANgS+ChS579Mg4EJgc2AM8DxwToH4/vMXOPBmYGGKt5a3AQ8A6wPfAf43N6K7DJgOjAZOJRvZdcVhaZtNgC2A29LxjALuA75eVf99QCuwI3AQcHQqPyq99gbGAcNYvR/2JDvW/YA9UtnI1A+3AQK+BWyc6m2Wjinvg8D+wFhgO+CoiFgBHAAsyI1oFhTtgPSL/FpgVuqHdwEnSNovVXkV+CxZ/++S1n8SICIqx7F9F0dSHwK+CQwHpgJnAW8CJgBvTHGckup297NpnXDysXq5CPhA5S9IskTU3rmUl4GNgM0j4uWIuDUK3HQwIhZFxFUR8VxELCf7hbJn0QBTbFcDP4qIP7ZT7bGIuCAiXk3xbwRsKGkMsDNwSkS8FBFTgWuK7ju5MCIeiYilZKOHRyLiz2la8gpgh6r6Z0XE4oh4HPghcHgqPwL4fkQ8GhHPAl8BDtOqU2ynRsSK9IfBaiLi4Yi4KSJejIiFwPdZvS9/HBELImIxWcKY0MXjPTGNIJZIejqV7QxsEBGnp358FLiALDETETMi4vaIeCUi5gHn14irq34fEdMiYiXwInAs8NnUt8uBMyv7p5ufTeuck4/VRfplvBA4SNI4sl8yl7VT/bvAw8CNaVrpy0X2IWmopPPTdNMysumkkSo+j/+/wAMRcVYHdf5VWYiI59LiMLIRwuJcGcATBfdb8e/c8vM13g+rqp9v/7EUA+nnY1XrBpP9pV4oNkmvl/RrSf9MfXkJ2Wgj71+55edqxNeZsyNiZHpV2t6cbMqukpSWkI0uNkxxvSlNe/0rxXVmjbi6Kt8XGwBDgRm5/V+fyqGbn03rnJOP1dPFZCOejwA3RsS/a1WKiOUR8fmIGAccCHxO0rvS6ufIfjlUvCG3/HlgK+BtEfE6XptOqr7QYTXpl8hWwDFdOJ68J4FRkvKxbdbNtorKtz8GqExvLSD7JZ5f9wqrJrNoZ7niW6l8u9SXH6ZAP3bQXlFPAHNzSWlkRAyPiHen9ecB9wNbpri+2klcK8h9XiS9oUadfLxPkyX6bXL7H5GmZDv7bFoPOPlYPV0M7EM2rdHu5cuS3iPpjelcyjKyef5X0+qZwIckrSFpf1adchlO9otjiaRRrH6OpL39HQAcDxzc3jRUZyLiMaANOFXSWpJ2IfvlVE9fUHaRxWbAZ4DKOY5fAZ9VdgHEMLLRweUdXFW4EFhJdn6oYjjwLFlfbgJ8oQtx/RsYXbnIoYumA8skfUnSOunfeVtJO+fiWgY8K2lr4BM19p0/jlnANpImSBrC6uetVpGm3i4AfiDp9QCSNqmcc+rks2k94ORjdZPm6P8OrEvH50O2BP5M9svvNuDciJiS1n2G7Jf6ErJzG1fntvshsA7ZX6+3k02XFHEo2bTKfbkrtH5WcNu8I8hOgi8CziBLBi92o52ifg/MIEvIfyCbNgT4BfBLsmnHucALwKfbayRNFX4TmJammt4OnEZ2IcPS1PZviwYVEfeTJcBHU3uFr3ZL59IOJDt/NJfs3/LnQCWRnUh2gcBysiRRfVHBqcBFab8fjIgHgdPJPk8PkV1Q0JkvkU2t3Z6m9v5MNiqGjj+b1gPyuTOz3iHpcrIr+gqNwLrYdpBNPT3c222bNYJHPmbdJGlnZd+PGZSmBA9i1ZGZmbWjkd92Nuvv3kA2PTWa7Lsgn4iIfzQ2JLP+wdNuZmZWOk+7mZlZ6TztVtD6668fLS0tjQ7DzKxfmTFjxtMRsUF1uZNPQS0tLbS1tTU6DDOzfkXSY7XKPe1mZmal88inoPvmL2KnL1zc6DDMzEo147sf7bxSN3jkY2ZmpfPIx8zMCnn55ZeZP38+L7zwwmrrhgwZwqabbsqaa65ZqC0nHzMzK2T+/PkMHz6clpYWXnumIkQEixYtYv78+YwdO7ZQW552MzOzQl544QVGjx69SuIBkMTo0aNrjoja4+RjZmaFVSeezsrb4+RjZmalc/IxM7PS9ankI2mypEMaHYeZmdXW3s2ou3qT6j6VfMzMrO8aMmQIixYtWi3RVK52GzJkSOG2GnaptaSvkT2G+AmyR+fOqFo/D2iNiKcltQJnR8Re6Rn1PwFagQBOi4irJB0OfBUQ8IeI+JKkNcgeNVyp+4uI+IGkLYCfkj1K+Tng2PQoYDMza8emm27K/PnzWbhw4WrrKt/zKaohySclk/cDO6QY7qIq+XTga8DSiHhLamu99Mz4s4CdgGeAGyUdTJbYNomIbVPdkamNScDHI+IhSW8DzgXeWSPOicBEgLWGj+7OoZqZNY0111yz8Pd4OtOokc/uwO8j4nkASdd2Ydt9gMMqbyLiGUl7AFMiYmFq71JgD+AbwDhJPwH+QJaUhgG7AlfkLg1cu9aOImISWaJi3TeM9VP3zMx6SaOST5ELwl/htXNS+YlEkU2hddpeSkzbA/sBnwI+CJwALImICV2K2MzMek2jLjiYChwoaUgaify/GnXmkU2jQTZFV3EjcFzljaT1gDuAPSWtn87zHA7cIml9YFBEXEU2XbdjRCwD5kr6QNpeKUGZmVlJGpJ8IuJO4BpgFvBboA1YWlXtNOBHkm4FXs2VnwGsJ+luSbOAvSPiSeArwF9Tm3dFxO+BTYApkmYCk1MdyC50OCZtfw9wUO8fpZmZtUddvTa713YsDYuIZyUNBf4GTIyIuxoSTAHrvmFsbP2R0xodhplZqXr6PB9JMyKitbq8kXe1niRpPNn5nIv6cuIxM7Pe1bDkExEfatS+zcyssXyHAzMzK52Tj5mZlc7Jx8zMSufHaBf05k1H09bDqz7MzCzjkY+ZmZXOycfMzErn5GNmZqVz8jEzs9L5goOCXnryHh4//S2NDsPMChpzypxGh2Ad8MjHzMxK5+RjZmalc/IxM7PSOfmYmVnpnHzMzKx0Tj5mZlY6Jx8zMytdUyQfSS2S7u7mthtLurK3YzIzs/YN+C+ZRsQC4JBGx2FmNpA0xcgnGSzpIkmzJV0paaikeZLOlHSbpDZJO0q6QdIjkj4OPRs1mZlZ9zRT8tkKmBQR2wHLgE+m8iciYhfgVmAy2Sjn7cDpnTUoaWJKWm2LV7xan6jNzAagZko+T0TEtLR8CbB7Wr4m/ZwD3BERyyNiIfCCpJEdNRgRkyKiNSJaR627Rn2iNjMbgJop+UQ7719MP1fmlivvB/w5LzOzRmim5DNG0i5p+XBgaiODMTOz9jVT8rkPOFLSbGAUcF6D4zEzs3Y0xbRTRMwDxtdY1ZKrM5nsgoPK+8q6p4Ft6xWbmZmtrplGPmZm1k84+ZiZWemcfMzMrHROPmZmVjonHzMzK11TXO1WhrU22oYxp7Q1Ogwzs6bgkY+ZmZXOycfMzErn5GNmZqVz8jEzs9L5goOC7n/qfnb7yW6NDsOsqU379LTOK1lT8MjHzMxK5+RjZmalc/IxM7PSOfmYmVnpnHzMzKx0Tj5mZlY6Jx8zMyudk4+ZmZXOycfMzErXNMlH0tWSZki6R9LEVHaMpAclTZF0gaRzUvkGkq6SdGd6+dYFZmYlaqbb6xwdEYslrQPcKekPwNeAHYHlwM3ArFT3R8APImKqpDHADcCbqxtMSWwiwFrrrVXCIZiZDQzNlHyOl/S+tLwZ8BHglohYDCDpCuBNaf0+wHhJlW1fJ2l4RCzPNxgRk4BJAMPGDIs6x29mNmA0RfKRtBdZQtklIp6TNAV4gBqjmWRQqvt8ORGamVles5zzGQE8kxLP1sDbgaHAnpLWkzQYeH+u/o3AcZU3kiaUGq2Z2QDXLMnnemCwpNnAN4DbgX8CZwJ3AH8G7gWWpvrHA62SZku6F/h4+SGbmQ1cTTHtFhEvAgdUl0tqi4hJaeTzO7IRDxHxNHBouVGamVlFs4x82nOqpJnA3cBc4OoGx2NmZjTJyKc9EXFio2MwM7PVNfvIx8zM+iAnHzMzK52Tj5mZla6pz/n0pq1fvzXTPj2t0WGYmTUFj3zMzKx0Tj5mZlY6Jx8zMyudk4+ZmZXOycfMzErnq90KWv7AA9yyx56NDsOspj3/dkujQzDrEo98zMysdE4+ZmZWOicfMzMrnZOPmZmVrsvJJz2Wert6BGNmZgNDoeQjaYqk10kaBcwCLpT0/fqGZmZmzaroyGdERCwD/hu4MCJ2AvapX1ggqUXS3Wm5VdKP67k/MzMrT9Hv+QyWtBHwQeCkOsZTU0S0AW1F60sSoIhYWb+ozMysu4qOfE4HbgAeiYg7JY0DHupsozR6uV/SRZJmS7pS0lBJO0m6RdIMSTekxEYqnyXpNuBTuXb2knRdWt5A0k2S7pJ0vqTHJK2f9nWfpHOBu4DNJO0r6bZU9wpJw3L7WW3/ZmZWjkLJJyKuiIjtIuIT6f2jEfH+gvvYCpgUEdsBy8iSyk+AQ9L03S+Ab6a6FwLHR8QuHbT3deDmiNgR+B0wpmpfF0fEDsAK4GRgn1S3DficpDU72P8qJE2U1CapbenLLxc8XDMz60yhaTdJbwLOAzaMiG3T1W7vjYgzCmz+RERUnsJ2CfBVYFvgpmx2jDWAJyWNAEZGROU+Ib8EDqjR3u7A+wAi4npJz+TWPRYRt6fltwPjgWlpP2sBt5ElqNX2XyvwiJgETALYavjwKHCsZmZWQNFzPhcAXwDOB4iI2ZIuA4okn+pf2suBe6pHN5JG1qhbizpYt6Kq3k0RcXjVft5Sa/9mZlaeoud8hkbE9KqyVwpuO0ZS5Rf94cDtwAaVMklrStomIpYASyXtnuoe0U57U8kufEDSvsB67dS7HdhN0htT3aFpBPdArf0XPBYzM+sFRZPP05K2II1MJB1CO1NVNdwHHClpNjCKdL4FOEvSLGAmsGuq+zHgp+mCg+fbae80YF9Jd5FNyz1JNppaRUQsBI4CfpX2fTuwdUS81MH+zcysBIrofKYrXd02ieyX9DPAXOCIiHisk+1agOsiYtseR/pam2sDr0bEK2n0cl5ETOit9tuz1fDhMWmHHeu9G7Nu8SMVrK+SNCMiWqvLOz3nI2kQ0BoR+0haFxgUEauNNEo0BvhNiusl4NgGxmJmZt3QafKJiJWSjgN+ExErOqtfte08sivLek1EPATs0JttmplZuYqe87lJ0omSNpM0qvKqa2RmZta0il5qfXT6+alcWQDjejccMzMbCAoln4gYW+9AzMxs4Ch6h4OP1iqPiIt7N5y+a/hWW/mKIjOzXlJ02m3n3PIQ4F1kN+8cMMnHzMx6T9Fpt0/n36f7sP2yLhGZmVnT6/JjtJPngC17MxAzMxs4ip7zuZbXbvo5iOxu0VfUKygzM2tuRc/5nJ1bfoXs0QXz6xCPmZkNAEWTz7sj4kv5AklnVZc1s6fmL+Wcz1/b6DCsC4773oGNDsHM2lH0nM9/1Sir9aA3MzOzTnU48pH0CeCTwLj0WIKK4cC02luZmZl1rLNpt8uAPwHfAr6cK18eEYvrFpWZmTW1DpNPRCwFlpI9gRRJryf7kukwScMi4vH6h2hmZs2m0DkfSQdKeojsIXK3APPIRkRmZmZdVvSCgzOAtwMPppuMvguf8zEzs24qmnxejohFwCBJgyLir0DdH11tZmbNqej3fJZIGgbcClwq6SmyL5v2aZJagOsiolefpmpmZj1TdORzENn93E4ArgceAfwNPjMz65aid7VeIWlzYMuIuEjSUGCN+obWawZLugjYAXgQ+ChwL3A5sHeq86GIeLhB8ZmZDThFr3Y7FrgSOD8VbQJcXa+getlWwKSI2A5YRvalWYBlEfFW4Bzgh7U2lDRRUpuktmefW1pOtGZmA0DRabdPAbuR/fImIh4CXl+voHrZExFRuTLvEmD3tPyr3M9dam0YEZMiojUiWocNHVHnMM3MBo6iyefFiHip8kbSYF57xEJfVx1n1CjvL8diZtYUiiafWyR9FVhH0n+RPcunv9zieYykysjmcGBqWj409/O20qMyMxvAiiafLwMLgTnA/wB/BE6uV1C97D7gyHRj1FHAeal8bUl3AJ8BPtuo4MzMBqLO7mo9JiIej4iVwAXp1W9ExDyyp66uQhLATyPitLJjMjOzzkc+/7miTdJVdY7FzMwGiM6+56Pc8rh6BlKmiGhpdAxmZgNZZyMfXxFmZma9rrORz/aSlpGNgNZJy6T3ERGvq2t0ZmbWlDp7mFx/uYWOmZn1I0Xvaj3gvX7TERz3Pd9L1cysNxT9no+ZmVmvcfIxM7PSOfmYmVnpnHzMzKx0vuCgoCfnPsI3P3xIo8OwLjjpkisbHYKZtcMjHzMzK52Tj5mZlc7Jx8zMSufkY2ZmpXPyMTOz0jn5mJlZ6Zx8zMysdE2XfCTNk7R+jfK/NyIeMzNbXdMln/ZExK6NjsHMzDL9OvlIWlfSHyTNknS3pENz69aRdL2kY9P7Z9PPvSRNkXSlpPslXSpJ7e3DzMx6X79OPsD+wIKI2D4itgWuT+XDgGuByyLighrb7QCcAIwHxgG71Wpc0kRJbZLaVrzwYu9Hb2Y2QPX35DMH2EfSWZLeERFLU/nvgQsj4uJ2tpseEfMjYiUwE2ipVSkiJkVEa0S0rjtk7V4P3sxsoOrXySciHgR2IktC35J0Slo1DTigg+m0/DDmVXyDVTOzUvXr5CNpY+C5iLgEOBvYMa06BVgEnNuo2MzMrH39OvkAbwGmS5oJnASckVt3AjBE0ncaEpmZmbWrX083RcQNwA1VxS255Y/l6g5LP6cAU3Llx9UtQDMzq6m/j3zMzKwfcvIxM7PSOfmYmVnpnHzMzKx0Tj5mZla6fn21W5k2GrsFJ11yZaPDMDNrCh75mJlZ6Zx8zMysdE4+ZmZWOicfMzMrnS84KOiFJ5dz3zdvbnQYVuXNJ72z0SGYWTd45GNmZqVz8jEzs9I5+ZiZWemcfMzMrHROPmZmVjonHzMzK52Tj5mZlW5AJx9JEyS9u9FxmJkNNA1NPpLWaOT+gQmAk4+ZWcnqlnwktUi6X9JFkmZLulLSUEnzJJ0iaSrwAUlbSLpe0gxJt0raOm2/haTbJd0p6XRJz6byvSRNSe3dL+lSSUrrTkn175Y0KVc+RdJZkqZLelDSOyStBZwOHCpppqRD69UXZma2qnqPfLYCJkXEdsAy4JOp/IWI2D0ifg1MAj4dETsBJwLnpjo/An4UETsDC6ra3QE4ARgPjAN2S+XnRMTOEbEtsA7wntw2gyPirWm7r0fES8ApwOURMSEiLq8OXtJESW2S2havWNKTfjAzs5x6J58nImJaWr4E2D0tXw4gaRiwK3CFpJnA+cBGqc4uwBVp+bKqdqdHxPyIWAnMBFpS+d6S7pA0B3gnsE1um9+mnzNy9TsUEZMiojUiWketO7LIJmZmVkC9bywa7bxfkX4OApZExIQutvtibvlVYLCkIWSjptaIeELSqcCQGtu8im+oambWUPUe+YyRtEtaPhyYml8ZEcuAuZI+AKDM9mn17cD70/JhBfZVSTRPpxHVIQW2WQ4ML1DPzMx6Ub2Tz33AkZJmA6OA82rUOQI4RtIs4B7goFR+AvA5SdPJpuKWdrSjiFgCXADMAa4G7iwQ31+B8b7gwMysXPWefloZER+vKmvJv4mIucD+Nbb9J/D2iAhJhwFtqf4UYEpu++NyyycDJ1c3FBF75ZafrsQQEYuBnYsfjpmZ9Ya+fO5jJ+CcdLn0EuDoBsdjZma9pG7JJyLmAdv2YPtbge07rWhmZv3OgL69jpmZNYaTj5mZlc7Jx8zMSteXLzjoU4ZsNJw3n/TORodhZtYUPPIxM7PSOfmYmVnpnHzMzKx0Tj5mZlY6X3BQ0IIFCzj11FMbHcaA4v42a14e+ZiZWemcfMzMrHROPmZmVjonHzMzK52Tj5mZlc7Jx8zMSufkY2ZmpetTyUfS33u5vRZJd6flVkk/7s32zcyse/rUl0wjYtc6tt0GtNWrfTMzK66vjXyeTT/3kjRF0pWS7pd0qSSldd+WdK+k2ZLOTmWTJR1S3U5V23tJui4tnyrpF2kfj0o6vpwjNDMz6GMjnyo7ANsAC4BpwG6S7gXeB2wdESFpZA/a3xrYGxgOPCDpvIh4OV9B0kRgIsCIESN6sCszM8vrUyOfKtMjYn5ErARmAi3AMuAF4OeS/ht4rgft/yEiXoyIp4GngA2rK0TEpIhojYjWoUOH9mBXZmaW15eTz4u55VeBwRHxCvBW4CrgYOD6tP4V0rGk6bm1utN+TwM2M7Ni+nLyWY2kYcCIiPgjcAIwIa2aB+yUlg8C1iw/OjMzK6q//bU/HPi9pCGAgM+m8gtS+XTgL8CKBsVnZmYFKCIaHUO/sPHGG8fEiRMbHcaA4uf5mPV/kmZERGt1eb+adjMzs+bg5GNmZqVz8jEzs9I5+ZiZWemcfMzMrHS+2q2g1tbWaGvzfUnNzLrCV7uZmVmf4eRjZmal87RbQZKWAw80Oo4+ZH3g6UYH0Ue4L1bl/ljVQO+PzSNig+rC/nZ7nUZ6oNa85UAlqc39kXFfrMr9sSr3R22edjMzs9I5+ZiZWemcfIqb1OgA+hj3x2vcF6tyf6zK/VGDLzgwM7PSeeRjZmalc/IxM7PSDfjkI2l/SQ9IeljSl2usX1vS5Wn9HZJacuu+ksofkLRfmXHXS3f7Q1KLpOclzUyvn5Udez0U6I89JN0l6RVJh1StO1LSQ+l1ZHlR108P++PV3OfjmvKiro8CffE5SfdKmi3pL5I2z61rus9Gl0XEgH0BawCPAOOAtYBZwPiqOp8EfpaWDwMuT8vjU/21gbGpnTUafUwN7I8W4O5GH0MD+qMF2A64GDgkVz4KeDT9XC8tr9foY2pUf6R1zzb6GErui72BoWn5E7n/K0332ejOa6CPfN4KPBwRj0bES8CvgYOq6hwEXJSWrwTeJUmp/NcR8WJEzAUeTu31Zz3pj2bUaX9ExLyImA2srNp2P+CmiFgcEc8ANwH7lxF0HfWkP5pNkb74a0Q8l97eDmyalpvxs9FlAz35bAI8kXs/P5XVrBMRrwBLgdEFt+1vetIfAGMl/UPSLZLeUe9gS9CTf+OB+vnoyBBJbZJul3Rw74ZWuq72xTHAn7q5bVMa6LfXqfUXe/W15+3VKbJtf9OT/ngSGBMRiyTtBFwtaZuIWNbbQZaoJ//GA/Xz0ZExEbFA0jjgZklzIuKRXoqtbIX7QtKHgVZgz65u28wG+shnPrBZ7v2mwIL26kgaDIwAFhfctr/pdn+k6cdFABExg2w+/E11j7i+evJvPFA/H+2KiAXp56PAFGCH3gyuZIX6QtI+wEnAeyPixa5s2+wGevK5E9hS0lhJa5GdQK++CucaoHI1yiHAzZGdNbwGOCxd/TUW2BKYXlLc9dLt/pC0gaQ1ANJftluSnUjtz4r0R3tuAPaVtJ6k9YB9U1l/1u3+SP2wdlpeH9gNuLdukdZfp30haQfgfLLE81RuVTN+Nrqu0Vc8NPoFvBt4kOwv9ZNS2elkHxiAIcAVZBcUTAfG5bY9KW33AHBAo4+lkf0BvB+4h+yqn7uAAxt9LCX1x85kf8muABYB9+S2PTr108PAxxp9LI3sD2BXYE76fMwBjmn0sZTQF38G/g3MTK9rmvmz0dWXb69jZmalG+jTbmZm1gBOPmZmVjonHzMzK52Tj5mZlc7Jx8zMSufkY/1e1d2SZ+bvPN6FNkZK+mTvR/ef9o+SdE692m9nnwdLGl/mPnP73lDSdZJmpTs7/7ERcVjf5eRjzeD5iJiQe83rRhsjye7Y3SWVL9b2NenuEweT3X29EU4nu3nm9hExHljtkQNdlY7JmoSTjzUlSWtI+q6kO9PzVP4nlQ9Lz1a5S9IcSZU7EX8b2CKNnL4raS9J1+XaO0fSUWl5nqRTJE0FPiBpC0nXS5oh6VZJW3cS22RJ50n6q6RHJe0p6ReS7pM0OVfvWUnfS7H+RdIGqXxCujnnbEm/S9+SR9IUSWdKugX4EvBe4LvpmLaQdGzqj1mSrpI0NBfPjyX9PcVzSC6GL6Z+miXp26msyPFuRPZlUwAiu9N1R20WOabPpDtpXJWO405Ju3XU19aHNfpbrn751dMX8CqvfYv8d6lsInByWl4baCN77tJg4HWpfH2yb5iLqucRAXsB1+XenwMclZbnAV/MrfsLsGVafhvZLYeqY9KgHJEAAALmSURBVDwKOCctTya7BX/l0RzLgLeQ/TE4A5iQ6gVwRFo+Jbf9bGDPtHw68MO0PAU4N7fPyaz6jKHRueUzgE/n6l2R9j+e7FEBAAcAf+e1Z9KM6sLx7gcsAf5KdieQjTtps+gxXQbsnpbHAPc1+vPnV/deHsZaM3g+IiZUle0LbJf7K34E2f3m5gNnStqD7JkzmwAbdmOfl0M2kiK7dcwVeu2xRmsX2P7aiAhJc4B/R8Sc1N49ZIlwZorv8lT/EuC3kkYAIyPillR+EVniWCWudmwr6QyyKcZhrHo/sasjYiVwr6RKf+wDXBjpmTQRsbjo8UbEDcru8bc/WcL5h6Rt22mzK8e0DzA+t+/XSRoeEcs7OG7rg5x8rFmJ7C/7VW7YmKbONgB2ioiXJc0ju19dtVdYdVq6us6K9HMQsKRG8utM5Q7HK3PLlfft/b8sci+sFR2smwwcHBGzUj/sVSMeeO2W/6qxz8LHGxGLyUYql6UpzD3aabMz+WMaBOwSEc93sQ3rY3zOx5rVDcAnJK0JIOlNktYlGwE9lRLP3sDmqf5yYHhu+8fI/sJeO/1l/q5aO4nseUVzJX0g7UeStu+lYxhEdudwgA8BUyNiKfCMXntY30eAW2ptzOrHNBx4MvXJEQX2fyNwdO7c0KiixyvpnbnthgNbAI+302ZXjulG4Ljcfrqa9K2P8MjHmtXPyaav7lI2R7OQ7OqvS4FrJbWRTW3dDxDZQ/CmSbob+FNEfEHSb8jORTwE/KODfR0BnCfpZGBNsvM5s3rhGFYA20iaQfbE2ENT+ZHAz9Iv8EeBj7Wz/a+BCyQdT5bEvgbcQZZY57BqYlpNRFyffrm3SXoJ+CPwVYod707AOZIqI8ifR8Sd8J+EUd1m0WM6HvippNlkv7/+Bny8o+Owvsl3tTbroyQ9GxHDGh2HWT142s3MzErnkY+ZmZXOIx8zMyudk4+ZmZXOycfMzErn5GNmZqVz8jEzs9L9H0sL6AYOdXxiAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "# Creating a bar plot\n",
    "sns.barplot(x=feature_imp, y=feature_imp.index)\n",
    "# Add labels to your graph\n",
    "plt.xlabel('Feature Importance Score')\n",
    "plt.ylabel('Features')\n",
    "plt.title(\"Visualizing Important Features\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can generate model after removing the least important features and the accuracy will increase. This is because we removed misleading data and noise, resulting in an increased accuracy. A lesser amount of features also reduces the training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_Advantages_**:  \n",
    "•Random forests is considered as a highly accurate and robust method because of the number of decision trees participating in the process.  \n",
    "•It does not suffer from the overfitting problem. The main reason is that it takes the average of all the predictions, which cancels out the biases.  \n",
    "•The algorithm can be used in both classification and regression problems.  \n",
    "•Random forests can also handle missing values. There are two ways to handle these: using median values to replace continuous variables, and computing the proximity-weighted average of missing values.  \n",
    "•We can get the relative feature importance, which helps in selecting the most contributing features for the classifier.\n",
    "\n",
    "**_Disadvantages_**:  \n",
    "•Random forests is slow in generating predictions because it has multiple decision trees. Whenever it makes a prediction, all the trees in the forest have to make a prediction for the same given input and then perform voting on it. This whole process is time-consuming.   \n",
    "•The model is difficult to interpret compared to a decision tree, where you can easily make a decision by following the path in the tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Questionnaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Do Random Forest overfit?\n",
    "##### Show effect of Random Forest hyperparameter tuning on performance in python\n",
    "\n",
    "\n",
    "[Solution](https://github.com/ebi-byte/kt/blob/master/trees/Trees%20Questionnaire.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
